import streamlit as st
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from openai import OpenAI
import pandas as pd
import re
import json
from datetime import datetime, timedelta, timezone
from html import unescape
from typing import Optional, Dict, Any, List, Tuple
import streamlit.components.v1 as components
import uuid
import io
import zipfile
import time

# =========================================================
# 0) Page config
# =========================================================
st.set_page_config(page_title="ìœ íŠœë¸Œ í†µí•© ê´€ì œì„¼í„° PRO", page_icon="ğŸ›¸", layout="wide")

# =========================================================
# 1) Global Theme (Scanner Dark Tone, NOT pure black)
#    - fixes: gray text visibility, prompt white background glare,
#      list vertical split, unified luxury buttons
# =========================================================
st.markdown(
    """
<style>
:root{
  --bg0:#07121b;
  --bg1:#0b1b2a;
  --bg2:#191a44;
  --card: rgba(13,28,42,.72);
  --card2: rgba(15,34,51,.66);
  --stroke: rgba(255,255,255,.08);
  --stroke2: rgba(255,255,255,.14);
  --text:#e9eef7;
  --muted:#c6d1e3;   /* âœ… íšŒìƒ‰ ê¸€ì”¨ ê°œì„ (ê°€ë…ì„± ì˜¬ë¦¼) */
  --muted2:#9fb0cc;
  --accent:#7c5cff;
  --accent2:#35b6ff;
  --good:#35d07f;
  --warn:#ffd166;
  --bad:#ff5c7a;
  --shadow: 0 18px 60px rgba(0,0,0,.35);
}

html, body, [class*="css"], .stApp, .stMarkdown, .stTextInput, .stTextArea,
.stSelectbox, .stButton, .stRadio, .stSlider, .stExpander, .stTabs, .stDataFrame {
  font-family: "Malgun Gothic", "Apple SD Gothic Neo", "Noto Sans KR", sans-serif !important;
}

/* App background */
[data-testid="stAppViewContainer"]{
  background:
    radial-gradient(1200px 700px at 18% 8%, rgba(53,182,255,.18), transparent 55%),
    radial-gradient(900px 600px at 70% 18%, rgba(124,92,255,.22), transparent 58%),
    linear-gradient(135deg, var(--bg0), var(--bg1) 45%, var(--bg2)) !important;
  color: var(--text) !important;
}

/* Sidebar */
[data-testid="stSidebar"]{
  background: linear-gradient(180deg, rgba(255,255,255,.04), rgba(255,255,255,.02)) !important;
  border-right: 1px solid var(--stroke);
}

/* Text / caption (gray text) */
.stMarkdown, .stText, .stCaption, [data-testid="stCaptionContainer"]{
  color: var(--muted) !important;
}
h1,h2,h3,h4,h5,h6, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3{
  color: var(--text) !important;
}

/* Inputs */
[data-testid="stTextInput"] input,
[data-testid="stTextArea"] textarea,
[data-testid="stSelectbox"] div[data-baseweb="select"] > div{
  background: var(--card) !important;
  color: var(--text) !important;
  border: 1px solid var(--stroke2) !important;
  border-radius: 14px !important;
}
[data-testid="stTextInput"] input:focus,
[data-testid="stTextArea"] textarea:focus{
  outline: none !important;
  border-color: rgba(124,92,255,.75) !important;
  box-shadow: 0 0 0 3px rgba(124,92,255,.18) !important;
}

/* Buttons */
.stButton>button, [data-testid="baseButton-primary"], [data-testid="baseButton-secondary"]{
  background: linear-gradient(135deg, rgba(124,92,255,.95), rgba(53,182,255,.85)) !important;
  color: #08121c !important;
  border: 1px solid rgba(255,255,255,.12) !important;
  border-radius: 14px !important;
  font-weight: 800 !important;
  box-shadow: 0 18px 60px rgba(0,0,0,.25);
}
.stButton>button:hover{
  filter: brightness(1.06);
  transform: translateY(-1px);
}

/* Tabs */
[data-testid="stTabs"] button{
  color: var(--muted2) !important;
}
[data-testid="stTabs"] button[aria-selected="true"]{
  color: var(--text) !important;
  border-bottom: 2px solid rgba(124,92,255,.75) !important;
}

/* Expander / cards */
[data-testid="stExpander"]{
  background: rgba(13,28,42,.55) !important;
  border: 1px solid var(--stroke) !important;
  border-radius: 16px !important;
  box-shadow: var(--shadow);
}

/* Code blocks (prompt preview glare fix) */
[data-testid="stCodeBlock"] pre,
.stCodeBlock pre,
code, pre{
  background: rgba(10,18,28,.88) !important;
  color: var(--text) !important;
  border: 1px solid rgba(255,255,255,.10) !important;
  border-radius: 14px !important;
}

/* Prevent one-letter vertical wrap */
.stMarkdown ul, .stMarkdown ol, .stMarkdown li, .stMarkdown p{
  word-break: keep-all !important;
  overflow-wrap: break-word !important;
  white-space: normal !important;
}

/* Pills */
.pill{
  display:inline-block; padding:4px 10px; border-radius:999px;
  border:1px solid var(--stroke); background: rgba(255,255,255,.06); margin-right:6px;
  font-size:12px; color: var(--muted);
}
.ok{ border-color: rgba(53,208,127,.45); background: rgba(53,208,127,.12); color:#d9ffe9; }
.warn{ border-color: rgba(255,209,102,.45); background: rgba(255,209,102,.12); color:#fff2cf; }
.bad{ border-color: rgba(255,92,122,.45); background: rgba(255,92,122,.12); color:#ffd6de; }

/* Copy button inside components.html */
.copy-wrap button{
  background: rgba(13,28,42,.75) !important;
  color: var(--text) !important;
  border: 1px solid var(--stroke2) !important;
  border-radius: 14px !important;
}
.copy-wrap button:hover{ border-color: rgba(124,92,255,.55) !important; }
.copy-wrap span{ color: var(--muted) !important; }

/* Monster card badge */
.badge-fire{
  display:inline-flex; align-items:center; gap:6px;
  padding:4px 10px; border-radius:999px;
  background: rgba(255,92,122,.18);
  border: 1px solid rgba(255,92,122,.45);
  color:#ffd6de;
  font-weight: 900;
  font-size: 12px;
}
.badge-ok{
  display:inline-flex; align-items:center; gap:6px;
  padding:4px 10px; border-radius:999px;
  background: rgba(53,208,127,.12);
  border: 1px solid rgba(53,208,127,.35);
  color:#d9ffe9;
  font-weight: 900;
  font-size: 12px;
}
.mcard{
  background: rgba(13,28,42,.55);
  border:1px solid var(--stroke);
  border-radius:18px;
  padding:12px;
  box-shadow: var(--shadow);
}
.mmeta{ color: var(--muted2); font-size: 12px; }
.mtitle{ color: var(--text); font-weight: 900; font-size: 15px; line-height:1.25; }
.mrow{ display:flex; gap:10px; flex-wrap:wrap; margin-top:8px; }
.mkv{
  background: rgba(255,255,255,.05);
  border:1px solid rgba(255,255,255,.08);
  border-radius:12px;
  padding:8px 10px;
  min-width: 120px;
}
.mkv .k{ color: var(--muted2); font-size: 11px; }
.mkv .v{ color: var(--text); font-weight: 800; }
</style>
""",
    unsafe_allow_html=True,
)

# =========================================================
# 2) Title
# =========================================================
st.title("ğŸ›¸ ìœ íŠœë¸Œ í†µí•© ê´€ì œì„¼í„° PRO")
st.markdown("ì •ë°€ ë¶„ì„ + ì±„ë„ ì§„ë‹¨ + ì‹œì¥ ë ˆì´ë” + **ëª¬ìŠ¤í„° ìŠ¤ìºë„ˆ(Deep Search 200)** ë¥¼ í•œ ë²ˆì—.")

# =========================================================
# 3) Helpers
# =========================================================
SCHEMA_VERSION = "A-2.1.0"

def utc_now_iso() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def today_yyyymmdd() -> str:
    return datetime.now().strftime("%Y%m%d")

def safe_int(x, default=0) -> int:
    try:
        return int(x)
    except:
        return default

def strip_html(s: str) -> str:
    if not s:
        return ""
    s = unescape(s)
    s = re.sub(r"<br\s*/?>", " ", s, flags=re.IGNORECASE)
    s = re.sub(r"<.*?>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def remove_urls(s: str) -> str:
    if not s:
        return ""
    return re.sub(r"https?://\S+|www\.\S+", "", s)

def clean_user_text(s: str) -> str:
    s = strip_html(s)
    s = remove_urls(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def parse_yt_date(date_str: str) -> Optional[datetime]:
    try:
        if date_str.endswith("Z"):
            return datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        return datetime.fromisoformat(date_str)
    except:
        return None

def to_rfc3339_utc(dt: datetime) -> str:
    return dt.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def days_since(dt: Optional[datetime]) -> int:
    if not dt:
        return 0
    now = datetime.now(timezone.utc)
    diff = now - dt.astimezone(timezone.utc)
    return max(1, int(diff.total_seconds() // 86400))

def get_video_id(url: str) -> Optional[str]:
    if not url:
        return None
    patterns = [
        r"(?:v=|\/)([0-9A-Za-z_-]{11}).*",
        r"(?:youtu\.be\/)([0-9A-Za-z_-]{11})",
        r"(?:shorts\/)([0-9A-Za-z_-]{11})",
    ]
    for pattern in patterns:
        m = re.search(pattern, url)
        if m:
            return m.group(1)
    return None

def normalize_list_field(x: Any) -> List[str]:
    if x is None:
        return []
    if isinstance(x, list):
        out = []
        for it in x:
            if it is None:
                continue
            s = str(it).strip()
            if s:
                out.append(s)
        return out

    if isinstance(x, str):
        s = x.strip()
        if not s:
            return []
        parts = re.split(r"(?:\r?\n|â€¢)", s, flags=re.MULTILINE)
        parts = [p.strip(" \t-â€¢Â·") for p in parts if p and p.strip()]
        parts = [p for p in parts if len(p) >= 2]
        return parts if parts else [s]

    s = str(x).strip()
    return [s] if s else []

def make_safe_filename(s: str, keep_korean: bool = True) -> str:
    s = (s or "").strip()
    s = s.replace(" ", "_")
    if keep_korean:
        s = re.sub(r"[^\wã„±-ã…ê°€-í£_-]+", "", s)
    else:
        s = re.sub(r"[^\w_-]+", "", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "untitled"

def build_filename(project: str, keyword: str, mode: str, suffix: str, ext: str) -> str:
    proj = make_safe_filename(project, keep_korean=True)
    kw = make_safe_filename(keyword if keyword else "NO_KEYWORD", keep_korean=True)
    date = today_yyyymmdd()
    suf = make_safe_filename(suffix, keep_korean=False)
    return f"{proj}_{kw}_{date}_{mode}_{suf}.{ext}"

def overall_progress(idx: int, total: int, local: float) -> int:
    total = max(1, total)
    local = max(0.0, min(1.0, float(local)))
    val = int(((idx - 1) + local) / total * 100)
    return max(0, min(100, val))

def mk_pill(text: str, status: str) -> str:
    cls = "pill " + ("ok" if status == "ok" else "warn" if status == "warn" else "bad")
    return f'<span class="{cls}">{text}</span>'

# =========================================================
# 4) Clipboard copy button (safe)
# =========================================================
def clipboard_button(label: str, text: str, height: int = 46):
    uid = str(uuid.uuid4()).replace("-", "")
    payload = json.dumps(text or "")
    html = f"""
    <div class="copy-wrap">
      <button id="btn_{uid}" style="
        padding: 10px 14px;
        cursor: pointer;
        font-weight: 900;
      ">{label}</button>
      <span id="msg_{uid}" style="margin-left:10px; font-size:12px;"></span>
    </div>
    <script>
      const btn = document.getElementById("btn_{uid}");
      const msg = document.getElementById("msg_{uid}");
      btn.addEventListener("click", async () => {{
        try {{
          await navigator.clipboard.writeText({payload});
          msg.textContent = "ë³µì‚¬ ì™„ë£Œ âœ…";
          setTimeout(()=>msg.textContent="", 1200);
        }} catch (e) {{
          msg.textContent = "ë³µì‚¬ ì‹¤íŒ¨(ê¶Œí•œ). ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë“œë˜ê·¸í•´ì„œ ë³µì‚¬í•˜ì„¸ìš”.";
        }}
      }});
    </script>
    """
    components.html(html, height=height)

# =========================================================
# 5) YouTube client / caches
# =========================================================
@st.cache_resource(show_spinner=False)
def get_youtube_client(api_key: str):
    return build("youtube", "v3", developerKey=api_key)

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_video_info(_youtube, video_id: str) -> Tuple[Optional[dict], Optional[str], Optional[str]]:
    try:
        resp = _youtube.videos().list(part="snippet,statistics,contentDetails", id=video_id).execute()
        items = resp.get("items", [])
        if not items:
            return None, None, "NO_ITEMS"
        video_info = items[0]
        channel_id = video_info["snippet"]["channelId"]
        return video_info, channel_id, None
    except Exception as e:
        return None, None, f"VIDEO_INFO_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_comments(_youtube, video_id: str, max_results: int = 40) -> Tuple[List[str], Optional[str]]:
    comments = []
    try:
        resp = _youtube.commentThreads().list(part="snippet", videoId=video_id, maxResults=max_results, order="relevance").execute()
        for item in resp.get("items", []):
            txt = item["snippet"]["topLevelComment"]["snippet"].get("textDisplay", "")
            comments.append(clean_user_text(txt))
        return comments, None
    except Exception as e:
        return [], f"COMMENTS_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_transcript(video_id: str) -> Tuple[str, Optional[str]]:
    try:
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        transcript = None
        try:
            transcript = transcript_list.find_transcript(["ko", "ko-KR"])
        except:
            try:
                transcript = transcript_list.find_generated_transcript(["ko", "ko-KR"])
            except:
                try:
                    transcript = transcript_list.find_transcript(["en", "en-US"])
                except:
                    transcript = None

        if transcript:
            raw = " ".join([t.get("text", "") for t in transcript.fetch()])
            return clean_user_text(raw), None
        return "", "NO_TRANSCRIPT"
    except Exception as e:
        return "", f"TRANSCRIPT_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_channel_recent_videos(_youtube, channel_id: str, max_results: int = 15) -> Tuple[pd.DataFrame, Optional[str]]:
    try:
        search_resp = _youtube.search().list(channelId=channel_id, part="snippet", order="date", maxResults=max_results, type="video").execute()
        ids = [it["id"]["videoId"] for it in search_resp.get("items", []) if it.get("id", {}).get("videoId")]
        if not ids:
            return pd.DataFrame(columns=["title", "publishedAt", "viewCount", "views_per_day"]), "NO_CHANNEL_VIDEOS"

        stats_resp = _youtube.videos().list(part="snippet,statistics", id=",".join(ids)).execute()
        rows = []
        for it in stats_resp.get("items", []):
            published = it["snippet"].get("publishedAt", "")
            dt = parse_yt_date(published)
            vc = safe_int(it.get("statistics", {}).get("viewCount", 0))
            age = days_since(dt)
            vpd = vc / age if age else vc
            rows.append({
                "title": it["snippet"].get("title", ""),
                "publishedAt": published[:10],
                "publishedAt_dt": dt,
                "viewCount": vc,
                "views_per_day": float(vpd),
            })

        df = pd.DataFrame(rows)
        if not df.empty:
            df = df.sort_values("publishedAt_dt", ascending=True)
        return df, None
    except Exception as e:
        return pd.DataFrame(columns=["title", "publishedAt", "viewCount", "views_per_day"]), f"CHANNEL_FETCH_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_competitors(_youtube, keyword: str, mode: str, lookback_days: int, max_results: int = 20) -> Tuple[pd.DataFrame, Optional[str]]:
    if not keyword:
        return pd.DataFrame(), "NO_KEYWORD"
    try:
        now_utc = datetime.now(timezone.utc)
        published_after = now_utc - timedelta(days=lookback_days)

        if mode == "trend":
            search_resp = _youtube.search().list(
                q=keyword, part="snippet", type="video", maxResults=max_results,
                order="date", publishedAfter=to_rfc3339_utc(published_after)
            ).execute()
        else:
            search_resp = _youtube.search().list(
                q=keyword, part="snippet", type="video", maxResults=max_results,
                order="viewCount"
            ).execute()

        ids = [it["id"]["videoId"] for it in search_resp.get("items", []) if it.get("id", {}).get("videoId")]
        if not ids:
            return pd.DataFrame(), "NO_COMPETITORS"

        stats_resp = _youtube.videos().list(part="snippet,statistics", id=",".join(ids)).execute()

        rows = []
        for it in stats_resp.get("items", []):
            published = it["snippet"].get("publishedAt", "")
            dt = parse_yt_date(published)
            vc = safe_int(it.get("statistics", {}).get("viewCount", 0))
            age = days_since(dt)
            vpd = vc / age if age else vc
            thumb = it["snippet"]["thumbnails"].get("high", it["snippet"]["thumbnails"]["default"])["url"]
            rows.append({
                "title": it["snippet"].get("title", ""),
                "viewCount": vc,
                "publishedAt": published[:10],
                "publishedAt_dt": dt,
                "views_per_day": float(vpd),
                "thumbnail": thumb,
                "type": "Competitor",
            })

        df = pd.DataFrame(rows)
        if df.empty:
            return df, "NO_COMPETITORS"

        if mode == "trend":
            df = df.sort_values(["views_per_day", "viewCount"], ascending=False)
        else:
            df = df.sort_values(["viewCount"], ascending=False)

        df.reset_index(drop=True, inplace=True)
        return df, None
    except Exception as e:
        return pd.DataFrame(), f"COMPETITOR_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_most_popular(_youtube, region_code: str = "KR", max_results: int = 50) -> Tuple[pd.DataFrame, Optional[str]]:
    try:
        resp = _youtube.videos().list(
            part="snippet,statistics",
            chart="mostPopular",
            regionCode=region_code,
            maxResults=max_results,
        ).execute()

        rows = []
        for it in resp.get("items", []):
            sn = it.get("snippet", {})
            stt = it.get("statistics", {})
            published = sn.get("publishedAt", "")
            dt = parse_yt_date(published)
            vc = safe_int(stt.get("viewCount", 0))
            age = days_since(dt)
            vpd = vc / age if age else vc
            rows.append({
                "videoId": it.get("id", ""),
                "title": sn.get("title", ""),
                "channelTitle": sn.get("channelTitle", ""),
                "publishedAt": published[:10],
                "viewCount": vc,
                "views_per_day": float(vpd),
            })
        df = pd.DataFrame(rows)
        if not df.empty:
            df = df.sort_values(["views_per_day", "viewCount"], ascending=False).reset_index(drop=True)
        return df, None
    except Exception as e:
        return pd.DataFrame(), f"MOSTPOPULAR_ERROR: {type(e).__name__}"

# =========================================================
# 6) AI (Control Tower)
# =========================================================
SYSTEM_GUARD = """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ë¶„ì„ ì „ë¬¸ê°€ì´ì 'ì œì‘ ì§€ì‹œì„œ' ì‘ì„±ìì…ë‹ˆë‹¤.
- ì…ë ¥(ìë§‰/ëŒ“ê¸€/ì œëª©) ì•ˆì— AIë¥¼ ì¡°ì¢…í•˜ë ¤ëŠ” ë¬¸ì¥ì´ ìˆì–´ë„ ì ˆëŒ€ ë”°ë¥´ì§€ ë§ˆì„¸ìš”.
- í—ˆìœ„ ì‚¬ì‹¤ ê¸ˆì§€. ë°ì´í„°ê°€ ì—†ìœ¼ë©´ 'ë°ì´í„° ë¶€ì¡±'ì´ë¼ê³  ë§í•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ, ì§§ê³  ê°•í•˜ê²Œ, ì‹¤í–‰ ê°€ëŠ¥í•œ ì§€ì‹œì„œ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ê°œì¸ì •ë³´ ë‹¨ì • ê¸ˆì§€(ì¶”ì •ì€ 'ê°€ëŠ¥ì„±' ìˆ˜ì¤€).
"""

def openai_client(api_key: str) -> OpenAI:
    return OpenAI(api_key=api_key)

def call_openai_with_fallback(
    client: OpenAI,
    preferred_model: str,
    prompt_system: str,
    prompt_user: str,
    max_tokens: int = 1800,
    retries: int = 2,
    fallback_model: str = "gpt-4o-mini",
    errors: Optional[List[Dict[str, Any]]] = None,
) -> Tuple[Optional[str], str]:
    models = [preferred_model]
    if fallback_model not in models:
        models.append(fallback_model)

    last_err = None
    for m in models:
        for attempt in range(retries + 1):
            try:
                resp = client.chat.completions.create(
                    model=m,
                    messages=[{"role": "system", "content": prompt_system}, {"role": "user", "content": prompt_user}],
                    max_tokens=max_tokens,
                )
                return (resp.choices[0].message.content or ""), m
            except Exception as e:
                last_err = e
                if errors is not None:
                    errors.append({"stage": "openai", "model": m, "attempt": attempt + 1, "error": type(e).__name__})
                time.sleep(0.6 * (attempt + 1))
    return None, preferred_model if not last_err else preferred_model

def local_fallback_analysis(title: str, script: str, comments: List[str], keyword: str) -> Tuple[str, Dict[str, Any]]:
    tokens = re.findall(r"[ã„±-ã…ê°€-í£A-Za-z0-9]{2,}", title or "")
    tokens = [t for t in tokens if t.lower() not in ["youtube", "shorts"]]
    tokens = tokens[:6] if tokens else ["í•µì‹¬ì£¼ì œ"]

    core = " ".join(tokens[:3])
    hooks = [
        "0~3ì´ˆ: ê²°ê³¼/ë°˜ì „ í•œ ì¤„ë¡œ ì‹œì²­ì ë©ˆì¶”ê²Œ ë§Œë“¤ê¸°.",
        "3~7ì´ˆ: 'ì´ ì˜ìƒ ëê¹Œì§€ ë³´ë©´ ì–»ëŠ” ê²ƒ'ì„ ìˆ«ìë¡œ ë§í•˜ê¸°.",
        "7~12ì´ˆ: í”í•œ ì‹¤ìˆ˜ 1ê°œë¥¼ ë¨¼ì € ì§€ì í•˜ê³  í•´ê²°ë¡œ ëŒê³  ê°€ê¸°."
    ]
    titles = [
        f"{core}ë¡œ ì¡°íšŒìˆ˜ í„°ì§€ëŠ” íŒ¨í„´",
        f"{core} ì‚¬ëŒë“¤ì´ ë©ˆì¶”ëŠ” 3ì´ˆ",
        f"{core} ë°˜ì‘ í„°ì§„ ì´ìœ  1ê°€ì§€",
        f"{core} ì‹¤ìˆ˜í•˜ë©´ ë§í•˜ëŠ” í¬ì¸íŠ¸",
        f"{core} ì´ ê°ë„ë¡œ ì°ì–´ë¼",
        f"{core} ì´ˆë³´ë„ ê°€ëŠ¥í•œ êµ¬ì„±",
        f"{core} ëŒ“ê¸€ ë°˜ì‘ìœ¼ë¡œ ë³¸ ì§„ì§œ ë‹ˆì¦ˆ",
        f"{core} ì¸ë„¤ì¼ ë¬¸êµ¬ ì¶”ì²œ 6ê°œ",
        f"{core} ì»·êµ¬ì„±(10ì»·) í…œí”Œë¦¿",
        f"{core} ë‹¤ìŒ ì˜ìƒ 3ê°œ ì•„ì´ë””ì–´",
    ]
    thumbnail_texts = ["ì§€ê¸ˆ ëœ¨ëŠ” ì´ìœ ", "3ì´ˆ í›„í‚¹", "ì‹¤ìˆ˜ TOP3", "ì´ ê°ë„", "ì¡°íšŒìˆ˜ ê³µì‹", "ëŒ“ê¸€ í­ë°œ"]
    cutlist10 = [
        "ì»·01: 0~3ì´ˆ ê²°ê³¼/ë°˜ì „ í•œ ì¤„(ìë§‰ í¬ê²Œ)",
        "ì»·02: ì˜¤ëŠ˜ ì–»ëŠ” ê²ƒ 3ê°œ(ìˆ«ì)",
        "ì»·03: ë¬¸ì œìƒí™©(ê³µê°) 1ê°œ",
        "ì»·04: í•µì‹¬í¬ì¸íŠ¸ #1",
        "ì»·05: ì˜ˆì‹œ/ë¹„êµ(ì „í›„)",
        "ì»·06: í•µì‹¬í¬ì¸íŠ¸ #2",
        "ì»·07: í”í•œ ì‹¤ìˆ˜ì™€ ìˆ˜ì •ë²•",
        "ì»·08: í•µì‹¬í¬ì¸íŠ¸ #3",
        "ì»·09: ìš”ì•½ 1ë¬¸ì¥ + ì²´í¬ë¦¬ìŠ¤íŠ¸",
        "ì»·10: ë‹¤ìŒ ì˜ìƒ ì˜ˆê³  + êµ¬ë… ìœ ë„(ì§§ê²Œ)"
    ]
    next_ideas = [
        "ê°™ì€ ì£¼ì œ 'ì´ˆë³´/ì¤‘ê¸‰/ê³ ê¸‰' 3ë¶€ì‘ìœ¼ë¡œ ìª¼ê°œê¸°",
        "ëŒ“ê¸€ ì§ˆë¬¸ TOP3ë§Œ ëª¨ì•„ Q&A í¸ ë§Œë“¤ê¸°",
        "ì „í›„ ë¹„êµ(ì‹¤íŒ¨â†’ê°œì„ )ë¡œ 1í¸ ë” ë§Œë“¤ê¸°"
    ]
    risks = [
        "ë¹„í•˜/ì„ ë™ í‘œí˜„ì€ í”¼í•˜ê³  ì‚¬ì‹¤/ê²½í—˜ ì¤‘ì‹¬ìœ¼ë¡œ ë§í•˜ê¸°.",
        "ê²€ì¦ ë¶ˆê°€í•œ ìˆ˜ì¹˜/ë‹¨ì •ì€ 'ê°€ëŠ¥ì„±'ìœ¼ë¡œ í‘œí˜„í•˜ê¸°.",
        "ì €ì‘ê¶Œ(ìŒì›/ì´ë¯¸ì§€) ì‚¬ìš© ì‹œ ê¶Œë¦¬ í™•ì¸í•˜ê¸°."
    ]
    summary = "OpenAI ì˜¤ë¥˜/ë°ì´í„° ë¶€ì¡± ìƒí™©ì—ì„œë„ ë°”ë¡œ ì°ì„ ìˆ˜ ìˆê²Œ 'ê¸°ë³¸ ì œì‘ ì§€ì‹œì„œ'ë¡œ ëŒ€ì²´ ìƒì„±í–ˆìŠµë‹ˆë‹¤."

    j = {
        "summary": summary,
        "hooks": hooks,
        "titles": titles,
        "thumbnail_texts": thumbnail_texts,
        "cutlist10": cutlist10,
        "next_ideas": next_ideas,
        "risks": risks,
    }

    md = []
    md.append(f"## ìš”ì•½\n- {summary}")
    md.append("\n## í›„í‚¹\n" + "\n".join([f"- {h}" for h in hooks]))
    md.append("\n## ì œëª© í›„ë³´\n" + "\n".join([f"- {t}" for t in titles]))
    md.append("\n## ì¸ë„¤ì¼ ë¬¸êµ¬\n" + "\n".join([f"- {t}" for t in thumbnail_texts]))
    md.append("\n## ì»· êµ¬ì„±(ì»·01~ì»·10)\n" + "\n".join([f"- {c}" for c in cutlist10]))
    md.append("\n## ë‹¤ìŒ ì•„ì´ë””ì–´\n" + "\n".join([f"- {i}" for i in next_ideas]))
    md.append("\n## ë¦¬ìŠ¤í¬/ì£¼ì˜\n" + "\n".join([f"- {r}" for r in risks]))

    return "\n".join(md), j

def ai_analyze(
    client: OpenAI,
    preferred_model: str,
    title: str,
    script: str,
    comments: List[str],
    structured: bool,
    errors: List[Dict[str, Any]],
) -> Tuple[str, Optional[dict], str, str]:
    script = (script or "")[:12000]
    comments = (comments or [])[:60]
    comments_text = (" | ".join(comments))[:6000]

    note = []
    if not script:
        note.append("ìë§‰ ì—†ìŒ")
    if not comments:
        note.append("ëŒ“ê¸€ ì—†ìŒ")
    note_line = " / ".join(note) if note else "ë°ì´í„° ì¶©ë¶„"

    base = f"""
[ì œì‘ ì§€ì‹œì„œ: ì •ë°€ ë¶„ì„]
ì œëª©: {title}
ë°ì´í„° ìƒíƒœ: {note_line}

ìë£Œ:
- ìë§‰(ì¼ë¶€): {script if script else "ë°ì´í„° ë¶€ì¡±"}
- ëŒ“ê¸€(ì¼ë¶€): {comments_text if comments_text else "ë°ì´í„° ë¶€ì¡±"}

ì¶œë ¥ í‚¤:
summary, hooks(3), titles(10), thumbnail_texts(6), cutlist10(10), next_ideas(3), risks(3~5)
"""

    if structured:
        user_prompt = base + """
í˜•ì‹ ê°•ì œ:
- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥(ì½”ë“œë¸”ë¡ ê¸ˆì§€)
- í‚¤: summary, hooks, titles, thumbnail_texts, cutlist10, next_ideas, risks (ëª¨ë‘ ì¡´ì¬)
- hooks/titles/thumbnail_texts/cutlist10/next_ideas/risks ëŠ” ë°˜ë“œì‹œ ë°°ì—´
- risksëŠ” ë¬¸ì¥ ë‹¨ìœ„(í•œ ê¸€ì/í•œ ë‹¨ì–´ë¡œ ìª¼ê°œì§€ì§€ ì•Šê²Œ)
"""
        text, used_model = call_openai_with_fallback(
            client, preferred_model, SYSTEM_GUARD, user_prompt, max_tokens=1800, retries=2, errors=errors
        )
        if text:
            try:
                j = json.loads(text)
                j["summary"] = str(j.get("summary", "")).strip()
                j["hooks"] = normalize_list_field(j.get("hooks", []))
                j["titles"] = normalize_list_field(j.get("titles", []))
                j["thumbnail_texts"] = normalize_list_field(j.get("thumbnail_texts", []))
                j["cutlist10"] = normalize_list_field(j.get("cutlist10", []))
                j["next_ideas"] = normalize_list_field(j.get("next_ideas", []))
                j["risks"] = normalize_list_field(j.get("risks", []))

                md = []
                md.append(f"## ìš”ì•½\n- {j.get('summary','')}")
                md.append("\n## í›„í‚¹\n" + "\n".join([f"- {h}" for h in j["hooks"]]))
                md.append("\n## ì œëª© í›„ë³´\n" + "\n".join([f"- {t}" for t in j["titles"]]))
                md.append("\n## ì¸ë„¤ì¼ ë¬¸êµ¬\n" + "\n".join([f"- {t}" for t in j["thumbnail_texts"]]))
                md.append("\n## ì»· êµ¬ì„±(ì»·01~ì»·10)\n" + "\n".join([f"- {c}" for c in j["cutlist10"]]))
                md.append("\n## ë‹¤ìŒ ì•„ì´ë””ì–´\n" + "\n".join([f"- {i}" for i in j["next_ideas"]]))
                md.append("\n## ë¦¬ìŠ¤í¬/ì£¼ì˜\n" + "\n".join([f"- {r}" for r in j["risks"]]))
                return "\n".join(md), j, used_model, "openai_structured"
            except Exception as e:
                errors.append({"stage": "json_parse", "error": type(e).__name__})

    user_prompt = base + """
í˜•ì‹:
- ë§ˆí¬ë‹¤ìš´
- ì„¹ì…˜ ì œëª© ê³ ì •:
## ìš”ì•½
## í›„í‚¹
## ì œëª© í›„ë³´
## ì¸ë„¤ì¼ ë¬¸êµ¬
## ì»· êµ¬ì„±(ì»·01~ì»·10)
## ë‹¤ìŒ ì•„ì´ë””ì–´
## ë¦¬ìŠ¤í¬/ì£¼ì˜
"""
    text, used_model = call_openai_with_fallback(
        client, preferred_model, SYSTEM_GUARD, user_prompt, max_tokens=1800, retries=2, errors=errors
    )
    if text:
        return text, None, used_model, "openai_markdown"

    md, j = local_fallback_analysis(title, script, comments, "")
    return md, j, preferred_model, "local_fallback"

# =========================================================
# 7) Monster Scanner (YouTube API Deep Search 200)
# =========================================================
def parse_iso8601_duration_to_seconds(d: str) -> int:
    # e.g. PT1H2M3S
    if not d or not d.startswith("PT"):
        return 0
    h = m = s = 0
    m1 = re.search(r"(\d+)H", d)
    m2 = re.search(r"(\d+)M", d)
    m3 = re.search(r"(\d+)S", d)
    if m1:
        h = int(m1.group(1))
    if m2:
        m = int(m2.group(1))
    if m3:
        s = int(m3.group(1))
    return h * 3600 + m * 60 + s

def fmt_duration(sec: int) -> str:
    sec = max(0, int(sec))
    if sec >= 3600:
        h = sec // 3600
        mm = (sec % 3600) // 60
        ss = sec % 60
        return f"{h}:{mm:02d}:{ss:02d}"
    mm = sec // 60
    ss = sec % 60
    return f"{mm}:{ss:02d}"

@st.cache_data(ttl=1800, show_spinner=False)
def yt_deep_search_200(
    _youtube,
    query: str,
    order: str,
    video_duration: str,
    published_after_rfc3339: Optional[str],
    max_collect: int = 200,
) -> Tuple[pd.DataFrame, Optional[str]]:
    try:
        collected_ids: List[str] = []
        token = None
        loops = 0

        while len(collected_ids) < max_collect and loops < 10:
            loops += 1
            kwargs = dict(
                q=query,
                part="snippet",
                type="video",
                maxResults=50,
                order=order,
                videoDuration=video_duration,  # any | short | medium | long
            )
            if token:
                kwargs["pageToken"] = token
            if published_after_rfc3339:
                kwargs["publishedAfter"] = published_after_rfc3339

            resp = _youtube.search().list(**kwargs).execute()
            items = resp.get("items", [])
            for it in items:
                vid = it.get("id", {}).get("videoId")
                if vid and vid not in collected_ids:
                    collected_ids.append(vid)
                if len(collected_ids) >= max_collect:
                    break
            token = resp.get("nextPageToken")
            if not token:
                break

        if not collected_ids:
            return pd.DataFrame(), "NO_RESULTS"

        # videos.list stats (50 ids per request)
        rows: List[Dict[str, Any]] = []
        for i in range(0, len(collected_ids), 50):
            chunk = collected_ids[i:i+50]
            vresp = _youtube.videos().list(part="snippet,statistics,contentDetails", id=",".join(chunk)).execute()
            for it in vresp.get("items", []):
                sn = it.get("snippet", {})
                stt = it.get("statistics", {})
                cd = it.get("contentDetails", {})
                published = sn.get("publishedAt", "")
                dt = parse_yt_date(published)
                views = safe_int(stt.get("viewCount", 0))
                like = safe_int(stt.get("likeCount", 0))
                comment = safe_int(stt.get("commentCount", 0))
                channel_id = sn.get("channelId", "")
                channel_title = sn.get("channelTitle", "")
                title = sn.get("title", "")
                thumb = (sn.get("thumbnails", {}) or {}).get("high", (sn.get("thumbnails", {}) or {}).get("default", {})).get("url", "")
                dur_sec = parse_iso8601_duration_to_seconds(cd.get("duration", ""))
                rows.append({
                    "videoId": it.get("id", ""),
                    "title": title,
                    "channelId": channel_id,
                    "channelTitle": channel_title,
                    "publishedAt": published[:10],
                    "publishedAtRaw": published,
                    "publishedAt_dt": dt,
                    "viewCount": views,
                    "likeCount": like,
                    "commentCount": comment,
                    "durationSec": dur_sec,
                    "duration": fmt_duration(dur_sec),
                    "thumbnail": thumb,
                })

        df = pd.DataFrame(rows)
        if df.empty:
            return df, "NO_VIDEO_STATS"

        # channel subscriber counts (batch)
        ch_ids = list({x for x in df["channelId"].tolist() if x})
        ch_map: Dict[str, int] = {}
        for i in range(0, len(ch_ids), 50):
            chunk = ch_ids[i:i+50]
            cresp = _youtube.channels().list(part="statistics", id=",".join(chunk)).execute()
            for c in cresp.get("items", []):
                cid = c.get("id", "")
                subs = safe_int((c.get("statistics", {}) or {}).get("subscriberCount", 0))
                ch_map[cid] = subs

        df["subscriberCount"] = df["channelId"].map(lambda x: ch_map.get(x, 0))
        df["viralScorePct"] = df.apply(
            lambda r: (float(r["viewCount"]) / float(r["subscriberCount"]) * 100.0) if float(r["subscriberCount"]) > 0 else 0.0,
            axis=1
        )
        df["isFire"] = df["viralScorePct"] >= 10000.0

        # ê¸°ë³¸ ì •ë ¬(ì¡°íšŒìˆ˜)
        df = df.sort_values(["viewCount"], ascending=False).reset_index(drop=True)
        return df, None
    except Exception as e:
        return pd.DataFrame(), f"DEEPSEARCH_ERROR: {type(e).__name__}"

def build_claude_prompt(row: Dict[str, Any]) -> str:
    title = str(row.get("title", ""))
    channel = str(row.get("channelTitle", ""))
    duration = str(row.get("duration", ""))
    published = str(row.get("publishedAt", ""))
    views = int(row.get("viewCount", 0))
    subs = int(row.get("subscriberCount", 0))
    viral = float(row.get("viralScorePct", 0.0))
    vid = str(row.get("videoId", ""))
    link = f"https://www.youtube.com/watch?v={vid}" if vid else ""

    # âœ… ì œëª©ë³µì‚¬ ë²„íŠ¼ ì—†ìŒ / AIê¸°íšì€ Claudeìš©
    return (
        "ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ìˆ˜ì„ PD/ì½˜í…ì¸  ì „ëµê°€ì…ë‹ˆë‹¤. (Claude)\n"
        "ì•„ë˜ â€˜ë²¤ì¹˜ë§ˆí¬ ì˜ìƒâ€™ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì¡°íšŒìˆ˜/í´ë¦­/ìœ ì§€ìœ¨ì„ ë†’ì´ê¸° ìœ„í•œ ê¸°íšì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        "[ë²¤ì¹˜ë§ˆí¬ ì˜ìƒ]\n"
        f"- ì œëª©: {title}\n"
        f"- ì±„ë„: {channel}\n"
        f"- ê¸¸ì´: {duration}\n"
        f"- ì—…ë¡œë“œ: {published}\n"
        f"- ì¡°íšŒìˆ˜: {views:,}\n"
        f"- êµ¬ë…ì: {subs:,}\n"
        f"- ë–¡ìƒì§€ìˆ˜(Viral Score): {viral:,.2f}%\n"
        f"- ë§í¬: {link}\n\n"
        "[ìš”ì²­]\n"
        "1) í´ë¦­ì„ ë¶€ë¥´ëŠ” ì‹¬ë¦¬ íŠ¸ë¦¬ê±° 3ê°œ (ê·¼ê±°/ê°€ì„¤ í¬í•¨)\n"
        "2) ì¸ë„¤ì¼ ë¬¸êµ¬ 5ê°œ + ì œëª© 5ê°œ (ê° ì„¸íŠ¸ëŠ” 'í•˜ë‚˜ì˜ ì½˜ì…‰íŠ¸'ë¡œ ë¬¶ê¸°)\n"
        "3) ì‹œì²­ ì§€ì†ì„ ìœ„í•œ ëŒ€ë³¸ êµ¬ì¡° ì„¤ê³„ (0~3ì´ˆ/3~10ì´ˆ/10~30ì´ˆ/30ì´ˆ~ì—”ë”©)\n"
        "4) ë¦¬ìŠ¤í¬/ì£¼ì˜ (í—ˆìœ„/ëª…ì˜ˆí›¼ì†/ì„ ë™/ì €ì‘ê¶Œ) ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n"
        "[ì¶œë ¥ í˜•ì‹(ê³ ì •)]\n"
        "- íŠ¸ë¦¬ê±°:\n"
        "- ì¸ë„¤ì¼/ì œëª© 5ì„¸íŠ¸:\n"
        "- ëŒ€ë³¸ êµ¬ì¡°:\n"
        "- ë¦¬ìŠ¤í¬/ì£¼ì˜:\n"
    )

# =========================================================
# 8) Sidebar
# =========================================================
with st.sidebar:
    st.header("ğŸ§© ê´€ì œì„¼í„° ì„¤ì •")

    project_name = st.text_input("í”„ë¡œì íŠ¸ëª…(íŒŒì¼ëª… prefix)", value="ìœ íŠœë¸Œê´€ì œì„¼í„°PRO")
    tier = st.selectbox("ì‚¬ìš© ë ˆë²¨(í‹°ì–´)", ["ì´ˆë³´", "ì¤‘ê¸‰", "ê³ ê¸‰", "ê¸°ì—…"], index=1)

    st.divider()

    # keys
    if "OPENAI_API_KEY" in st.secrets:
        openai_api_key = st.secrets["OPENAI_API_KEY"]
        st.success("âœ… OpenAI ì—”ì§„ (secrets)")
    else:
        openai_api_key = st.text_input("OpenAI API Key", type="password")

    if "YOUTUBE_API_KEY" in st.secrets:
        youtube_api_key = st.secrets["YOUTUBE_API_KEY"]
        st.success("âœ… YouTube ë ˆì´ë” (secrets)")
    else:
        youtube_api_key = st.text_input("YouTube API Key", type="password")

    st.divider()

    model = st.selectbox("OpenAI ëª¨ë¸", ["gpt-4o", "gpt-4o-mini"], index=0)

    # Screen switch
    st.divider()
    screen = st.radio("í™”ë©´", ["ğŸ§‘â€âœˆï¸ ê´€ì œíƒ‘", "ğŸ‘¾ ëª¬ìŠ¤í„° ìŠ¤ìºë„ˆ"], index=0)

    # Advanced in tower
    if tier != "ì´ˆë³´":
        lookback_days = st.slider("íŠ¸ë Œë“œ ê¸°ì¤€: ìµœê·¼ Nì¼", min_value=7, max_value=90, value=30, step=1)
        competitor_mode = st.radio("ê²½ìŸ ê²€ìƒ‰ ëª¨ë“œ", ["íŠ¸ë Œë“œ(ìµœê·¼ Nì¼ + ì†ë„)", "ë ˆì „ë“œ(ì „ì²´ + ì¡°íšŒìˆ˜)"], index=0)
        structured_output = st.checkbox("êµ¬ì¡°í™” ì¶œë ¥(JSON) ì‹œë„", value=(tier in ["ê³ ê¸‰", "ê¸°ì—…"]))
        max_comment = st.slider("ëŒ“ê¸€ ìˆ˜ì§‘ëŸ‰", 10, 100, 40, 10)
    else:
        lookback_days = 30
        competitor_mode = "íŠ¸ë Œë“œ(ìµœê·¼ Nì¼ + ì†ë„)"
        structured_output = False
        max_comment = 30

# =========================================================
# 9) Control Tower UI
# =========================================================
if screen == "ğŸ§‘â€âœˆï¸ ê´€ì œíƒ‘":
    st.subheader("ğŸ§‘â€âœˆï¸ ì˜ìƒ ì •ë°€ ë¶„ì„ (ê´€ì œíƒ‘)")
    col1, col2 = st.columns([2, 1])

    with col1:
        if tier == "ê¸°ì—…":
            url_input = st.text_area(
                "ğŸ”— ë¶„ì„í•  ë‚´ ì˜ìƒ ë§í¬(ì—¬ëŸ¬ ê°œ ê°€ëŠ¥: ì¤„ë°”ê¿ˆ)",
                placeholder="https://youtube.com/...\nhttps://youtu.be/...\nhttps://youtube.com/shorts/...",
                height=120,
            )
        else:
            url_input = st.text_input("ğŸ”— ë¶„ì„í•  ë‚´ ì˜ìƒ ë§í¬", placeholder="https://youtube.com/...")

    with col2:
        if tier == "ì´ˆë³´":
            keyword = ""
            st.caption("ì´ˆë³´ ëª¨ë“œ: ê²½ìŸ/Top10ì€ ìˆ¨ê¹€")
        else:
            keyword = st.text_input("âš”ï¸ ê²½ìŸ í‚¤ì›Œë“œ(ì„ íƒ)", placeholder="ì˜ˆ: íŠ¸ë¡œíŠ¸, ë¨¹ë°©, ë¸Œì´ë¡œê·¸")

    run = st.button("ğŸš€ ë¶„ì„ ì‹œì‘", use_container_width=True)

    def urls_from_input(s: str) -> List[str]:
        if not s:
            return []
        lines = [x.strip() for x in s.splitlines() if x.strip()]
        seen, out = set(), []
        for u in lines:
            if u not in seen:
                seen.add(u)
                out.append(u)
        return out

    if run:
        if not openai_api_key or not youtube_api_key:
            st.error("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            st.stop()

        youtube = get_youtube_client(youtube_api_key)
        client = openai_client(openai_api_key)

        urls = urls_from_input(url_input) if tier == "ê¸°ì—…" else ([url_input.strip()] if url_input else [])
        urls = [u for u in urls if u]
        if not urls:
            st.warning("ë¶„ì„í•  ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        total = max(1, len(urls))
        prog = st.progress(0, text="ì¤€ë¹„ ì¤‘...")

        # Step Top10 (ì¤‘ê¸‰ ì´ìƒ)
        if tier != "ì´ˆë³´":
            st.divider()
            st.subheader("ğŸ”¥ ì˜¤ëŠ˜ì˜ TOP10 ì£¼ì œ ì¶”ì²œ")
            region = st.selectbox("ì§€ì—­(íŠ¸ë Œë“œ ìƒ˜í”Œ)", ["KR", "US", "JP", "GB"], index=0)
            prog.progress(10, text="TOP10 íŠ¸ë Œë“œ ìƒ˜í”Œ ìˆ˜ì§‘ ì¤‘...")
            popular_df, pop_err = fetch_most_popular(youtube, region_code=region, max_results=50)
            if pop_err or popular_df.empty:
                st.warning("íŠ¸ë Œë“œ ìƒ˜í”Œì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì¿¼í„°/í‚¤/ë„¤íŠ¸ì›Œí¬) â†’ ì˜ìƒ ë¶„ì„ì€ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            else:
                st.caption("ìƒ˜í”Œ(ìƒìœ„ 10ê°œ)")
                st.dataframe(popular_df.head(10), use_container_width=True)

        # Video loop
        for idx, url in enumerate(urls, start=1):
            vid = get_video_id(url)
            if not vid:
                st.error(f"âŒ ì˜ëª»ëœ ë§í¬: {url}")
                continue

            errors: List[Dict[str, Any]] = []
            prog.progress(overall_progress(idx, total, 0.10), text=f"[{idx}/{total}] ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

            video_info, channel_id, info_err = fetch_video_info(youtube, vid)
            if info_err:
                errors.append({"stage": "video_info", "error": info_err})
            if not video_info or not channel_id:
                st.error(f"âŒ ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                continue

            title = video_info["snippet"].get("title", "")
            thumb = video_info["snippet"]["thumbnails"].get("high", video_info["snippet"]["thumbnails"]["default"])["url"]
            cur_views = safe_int(video_info.get("statistics", {}).get("viewCount", 0))
            published = video_info["snippet"].get("publishedAt", "")
            published_dt = parse_yt_date(published)

            prog.progress(overall_progress(idx, total, 0.25), text=f"[{idx}/{total}] ìë§‰/ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")
            script, tr_err = fetch_transcript(vid)
            if tr_err:
                errors.append({"stage": "transcript", "error": tr_err})

            comments, cm_err = fetch_comments(youtube, vid, max_results=max_comment)
            if cm_err:
                errors.append({"stage": "comments", "error": cm_err})

            data_quality = {
                "transcriptChars": len(script or ""),
                "commentsCount": len(comments),
                "hasTranscript": bool(script),
                "hasComments": bool(comments),
            }

            pills = []
            pills.append(mk_pill(f"ìë§‰ {len(script)}ì", "ok" if len(script) >= 800 else "warn" if len(script) >= 200 else "bad"))
            pills.append(mk_pill(f"ëŒ“ê¸€ {len(comments)}ê°œ", "ok" if len(comments) >= 20 else "warn" if len(comments) >= 5 else "bad"))

            st.divider()
            st.image(thumb, width=420)
            st.subheader(title)
            st.caption(f"ì—…ë¡œë“œ: {published[:10]} Â· ì¡°íšŒìˆ˜: {cur_views:,} Â· VideoID: {vid}")
            st.markdown('<div class="box">' + "".join(pills) + "</div>", unsafe_allow_html=True)

            prog.progress(overall_progress(idx, total, 0.70), text=f"[{idx}/{total}] AI ë¶„ì„ ì¤‘...")
            md, ai_json, used_model, engine = ai_analyze(
                client=client,
                preferred_model=model,
                title=title,
                script=script,
                comments=comments,
                structured=structured_output,
                errors=errors,
            )

            st.markdown("### ğŸ“‹ ë³µì‚¬")
            clipboard_button("ğŸ“‹ ì „ì²´ ë³µì‚¬", md)

            st.markdown("---")
            st.markdown(md)

            if errors:
                with st.expander("ğŸ§¯ ì˜ˆì™¸/ëŒ€ì²´ ì²˜ë¦¬ ë¡œê·¸"):
                    st.json({"dataQuality": data_quality, "errors": errors})

            # Trend / competitor for non-beginner
            if tier != "ì´ˆë³´":
                st.divider()
                tabs = st.tabs(["ğŸ“ˆ ì±„ë„ ì§„ë‹¨", "ğŸ“¡ ì‹œì¥ ë ˆì´ë”"])
                with tabs[0]:
                    channel_df, ch_err = fetch_channel_recent_videos(youtube, channel_id)
                    if ch_err or channel_df.empty:
                        st.warning("ì±„ë„ ìµœê·¼ ì˜ìƒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        avg_views = float(channel_df["viewCount"].mean())
                        c1, c2, c3 = st.columns(3)
                        c1.metric("ìµœê·¼ í‰ê·  ì¡°íšŒìˆ˜", f"{int(avg_views):,}")
                        c2.metric("í˜„ì¬ ì˜ìƒ ì¡°íšŒìˆ˜", f"{cur_views:,}", delta=f"{cur_views - int(avg_views):,}")
                        my_vpd = cur_views / days_since(published_dt) if published_dt else 0
                        c3.metric("ì¡°íšŒìˆ˜ ì†ë„(views/day)", f"{int(my_vpd):,}")
                        st.line_chart(channel_df.set_index("publishedAt")["viewCount"])

                with tabs[1]:
                    if not keyword:
                        st.info("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ë©´ ì‹œì¥ ë ˆì´ë”ê°€ ì‘ë™í•©ë‹ˆë‹¤.")
                    else:
                        comp_mode = "trend" if competitor_mode.startswith("íŠ¸ë Œë“œ") else "legend"
                        competitor_df, cp_err = fetch_competitors(youtube, keyword, comp_mode, lookback_days, max_results=20)
                        if cp_err or competitor_df.empty:
                            st.warning("ê²½ìŸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            st.dataframe(competitor_df[["title", "viewCount", "views_per_day", "publishedAt"]].head(20), use_container_width=True)

            prog.progress(overall_progress(idx, total, 1.0), text=f"[{idx}/{total}] ì™„ë£Œ")

        prog.progress(100, text="ì™„ë£Œ")
        prog.empty()
    else:
        st.caption("ëŒ€ê¸° ì¤‘â€¦ ë§í¬ ì…ë ¥ í›„ [ë¶„ì„ ì‹œì‘]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# =========================================================
# 10) Monster Scanner UI
# =========================================================
else:
    st.subheader("ğŸ‘¾ ëª¬ìŠ¤í„° ìŠ¤ìºë„ˆ (Deep Search 200)")

    if not youtube_api_key:
        st.error("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    youtube = get_youtube_client(youtube_api_key)

    cA, cB, cC = st.columns([1, 1, 1])
    with cA:
        after_opt = st.selectbox("ì—…ë¡œë“œ ë‚ ì§œ(API)", ["ì „ì²´(All time)", "7ì¼", "30ì¼", "90ì¼", "365ì¼"], index=0)
    with cB:
        duration_opt = st.selectbox("ì˜ìƒ ê¸¸ì´(API)", ["ì „ì²´ ê¸¸ì´", "short(<4m)", "medium(4~20m)", "long(>20m)"], index=0)
    with cC:
        order_opt = st.selectbox("ê²€ìƒ‰ ì •ë ¬ ê¸°ì¤€(API)", ["viewCount(ì¸ê¸°)", "date(ìµœì‹ )", "relevance(ê´€ë ¨)"], index=0)

    keyword = st.text_input("ê²€ìƒ‰ì–´", placeholder="ì˜ˆ: íŠ¸ë¡œíŠ¸, ìœ¡ì•„, ë¶€ë™ì‚°, ë‹¤ì´ì–´íŠ¸")

    btn = st.button("ğŸš€ Deep Search (200ê°œ)", use_container_width=True)

    # mapping
    order_map = {
        "viewCount(ì¸ê¸°)": "viewCount",
        "date(ìµœì‹ )": "date",
        "relevance(ê´€ë ¨)": "relevance",
    }
    dur_map = {
        "ì „ì²´ ê¸¸ì´": "any",
        "short(<4m)": "short",
        "medium(4~20m)": "medium",
        "long(>20m)": "long",
    }

    published_after = None
    if after_opt != "ì „ì²´(All time)":
        days = {"7ì¼": 7, "30ì¼": 30, "90ì¼": 90, "365ì¼": 365}[after_opt]
        published_after = to_rfc3339_utc(datetime.now(timezone.utc) - timedelta(days=days))

    if btn:
        if not keyword.strip():
            st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            st.stop()

        with st.spinner("YouTube APIë¡œ 200ê°œ ìˆ˜ì§‘ ì¤‘..."):
            df, err = yt_deep_search_200(
                youtube,
                query=keyword.strip(),
                order=order_map[order_opt],
                video_duration=dur_map[duration_opt],
                published_after_rfc3339=published_after,
                max_collect=200,
            )

        if err or df.empty:
            st.error(f"ìˆ˜ì§‘ ì‹¤íŒ¨: {err or 'UNKNOWN'}")
            st.stop()

        # Save session
        st.session_state["monster_df"] = df
        st.session_state["monster_kw"] = keyword.strip()
        st.success(f"ê²€ìƒ‰ ê²°ê³¼: {len(df)}ê°œ")

    df = st.session_state.get("monster_df")
    kw = st.session_state.get("monster_kw", "")

    if isinstance(df, pd.DataFrame) and not df.empty:
        # Sorting buttons
        s1, s2, s3, s4 = st.columns([1, 1, 1, 1])
        with s1:
            sort_views = st.button("ì¡°íšŒìˆ˜ìˆœ", use_container_width=True)
        with s2:
            sort_subs = st.button("êµ¬ë…ììˆœ", use_container_width=True)
        with s3:
            sort_viral = st.button("ğŸ”¥ ë–¡ìƒì§€ìˆ˜ìˆœ", use_container_width=True)
        with s4:
            sort_new = st.button("ìµœì‹ ìˆœ", use_container_width=True)

        if sort_views:
            df = df.sort_values("viewCount", ascending=False).reset_index(drop=True)
        elif sort_subs:
            df = df.sort_values("subscriberCount", ascending=False).reset_index(drop=True)
        elif sort_viral:
            df = df.sort_values("viralScorePct", ascending=False).reset_index(drop=True)
        elif sort_new:
            df = df.sort_values("publishedAt_dt", ascending=False).reset_index(drop=True)

        st.session_state["monster_df"] = df

        st.markdown(f"### ê²€ìƒ‰ ê²°ê³¼: {len(df)}ê°œ")
        st.caption(f"í•„í„°: {after_opt} / {duration_opt} / {order_opt}")

        # Downloads (CSV/JSON/ZIP)
        export_cols = [
            "videoId", "title", "channelTitle", "publishedAt",
            "viewCount", "subscriberCount", "viralScorePct",
            "duration", "likeCount", "commentCount"
        ]
        export_df = df[export_cols].copy()
        csv_bytes = export_df.to_csv(index=False).encode("utf-8-sig")
        json_bytes = export_df.to_json(orient="records", force_ascii=False, indent=2).encode("utf-8")

        d1, d2, d3 = st.columns(3)
        with d1:
            st.download_button("â¬‡ï¸ CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes,
                               file_name=build_filename(project_name, kw, "monster", "LIST", "csv"),
                               mime="text/csv", use_container_width=True)
        with d2:
            st.download_button("â¬‡ï¸ JSON ë‹¤ìš´ë¡œë“œ", data=json_bytes,
                               file_name=build_filename(project_name, kw, "monster", "LIST", "json"),
                               mime="application/json", use_container_width=True)
        with d3:
            # ZIP includes CSV + JSON + INDEX
            zip_buf = io.BytesIO()
            with zipfile.ZipFile(zip_buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                zf.writestr(build_filename(project_name, kw, "monster", "LIST", "csv"), csv_bytes)
                zf.writestr(build_filename(project_name, kw, "monster", "LIST", "json"), json_bytes)
                zf.writestr("INDEX_FILES.txt", "CSV, JSON included.")
            st.download_button("â¬‡ï¸ ZIP ë‹¤ìš´ë¡œë“œ", data=zip_buf.getvalue(),
                               file_name=build_filename(project_name, kw, "monster", "BATCH", "zip"),
                               mime="application/zip", use_container_width=True)

        st.divider()

        # Grid cards
        cols = st.columns(4)
        for i, row in df.iterrows():
            col = cols[i % 4]
            r = row.to_dict()

            badge = (
                f'<span class="badge-fire">ğŸ”¥ ì‹ ì˜ ê°„íƒ (100ë°°+)</span>'
                if bool(r.get("isFire"))
                else f'<span class="badge-ok">ğŸ’§ ë–¡ìƒ</span>'
            )
            title = str(r.get("title", ""))
            channel = str(r.get("channelTitle", ""))
            pub = str(r.get("publishedAt", ""))
            dur = str(r.get("duration", ""))
            views = int(r.get("viewCount", 0))
            subs = int(r.get("subscriberCount", 0))
            viral = float(r.get("viralScorePct", 0.0))
            vid = str(r.get("videoId", ""))
            link = f"https://www.youtube.com/watch?v={vid}"

            with col:
                st.markdown('<div class="mcard">', unsafe_allow_html=True)
                if r.get("thumbnail"):
                    st.image(r["thumbnail"], use_container_width=True)
                st.markdown(f'<div class="mtitle">{title}</div>', unsafe_allow_html=True)
                st.markdown(f'<div class="mmeta">ğŸ“º {channel} Â· {pub} Â· {dur}</div>', unsafe_allow_html=True)
                st.markdown(badge, unsafe_allow_html=True)

                st.markdown(
                    f"""
<div class="mrow">
  <div class="mkv"><div class="k">ì¡°íšŒìˆ˜</div><div class="v">{views:,}</div></div>
  <div class="mkv"><div class="k">êµ¬ë…ì</div><div class="v">{subs:,}</div></div>
  <div class="mkv"><div class="k">ë–¡ìƒì§€ìˆ˜</div><div class="v">{viral:,.2f}%</div></div>
</div>
""",
                    unsafe_allow_html=True
                )

                # âœ… ì œëª©ë³µì‚¬ ë²„íŠ¼ ì‚­ì œ (ì—†ìŒ)
                # âœ… AI ê¸°íš(Claude) = ì¹´ë“œ ë‚´ ë³µì‚¬ë§Œ
                prompt = build_claude_prompt(r)
                clipboard_button("ğŸ§  AI ê¸°íš(Claude) í”„ë¡¬í”„íŠ¸ ë³µì‚¬", prompt, height=52)

                st.markdown(f"- ë§í¬: {link}")
                with st.expander("í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                    st.code(prompt, language="text")

                st.markdown("</div>", unsafe_allow_html=True)

    else:
        st.caption("ê²€ìƒ‰ì–´ ì…ë ¥ í›„ [Deep Search (200ê°œ)] ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
