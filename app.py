import streamlit as st
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from openai import OpenAI
import pandas as pd
import re
import json
from datetime import datetime, timedelta, timezone
from html import unescape
from typing import Optional, Dict, Any, List, Tuple
import streamlit.components.v1 as components
import uuid
import io
import zipfile
import time

# =========================================================
# 1) Page config & style (âœ… Malgun Gothic)
# =========================================================
st.set_page_config(page_title="ìœ íŠœë¸Œ í†µí•© ê´€ì œì„¼í„° PRO", page_icon="ğŸ›¸", layout="wide")

st.markdown(
    """
    <style>
      html, body, [class*="css"], .stApp, .stMarkdown, .stTextInput, .stTextArea,
      .stSelectbox, .stButton, .stRadio, .stSlider, .stExpander, .stTabs, .stDataFrame {
        font-family: "Malgun Gothic", "Apple SD Gothic Neo", "Noto Sans KR", sans-serif !important;
      }
      .copy-wrap { margin-top: 6px; margin-bottom: 6px; }
      .tiny { font-size:12px; color: #666; }
      .pill {
        display:inline-block; padding:4px 10px; border-radius:999px;
        border:1px solid #e5e7eb; background:#f8fafc; margin-right:6px;
        font-size:12px;
      }
      .ok { border-color:#86efac; background:#f0fdf4; }
      .warn { border-color:#fde68a; background:#fffbeb; }
      .bad { border-color:#fecaca; background:#fef2f2; }
      .box {
        border:1px solid #e5e7eb; border-radius:14px; padding:12px 14px;
        background:white;
      }
    </style>
    """,
    unsafe_allow_html=True,
)

st.title("ğŸ›¸ ìœ íŠœë¸Œ í†µí•© ê´€ì œì„¼í„° PRO")
st.markdown("ì •ë°€ ë¶„ì„ + ì±„ë„ ì§„ë‹¨ + ì‹œì¥ ë ˆì´ë” + **ì˜¤ëŠ˜ì˜ TOP10 ì£¼ì œ** + ë°°ì¹˜ ZIPê¹Œì§€ í•œ ë²ˆì—.")

# =========================================================
# 2) Helpers
# =========================================================
SCHEMA_VERSION = "A-1.1.0"  # Step A5

def utc_now_iso() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def today_yyyymmdd() -> str:
    return datetime.now().strftime("%Y%m%d")

def clamp_text(s: str, max_chars: int) -> str:
    s = s or ""
    return s if len(s) <= max_chars else s[:max_chars] + "..."

def safe_int(x, default=0) -> int:
    try:
        return int(x)
    except:
        return default

def strip_html(s: str) -> str:
    if not s:
        return ""
    s = unescape(s)
    s = re.sub(r"<br\s*/?>", " ", s, flags=re.IGNORECASE)
    s = re.sub(r"<.*?>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def remove_urls(s: str) -> str:
    if not s:
        return ""
    return re.sub(r"https?://\S+|www\.\S+", "", s)

def clean_user_text(s: str) -> str:
    s = strip_html(s)
    s = remove_urls(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def parse_yt_date(date_str: str) -> Optional[datetime]:
    try:
        if date_str.endswith("Z"):
            return datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        return datetime.fromisoformat(date_str)
    except:
        return None

def to_rfc3339_utc(dt: datetime) -> str:
    return dt.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def days_since(dt: Optional[datetime]) -> int:
    if not dt:
        return 0
    now = datetime.now(timezone.utc)
    diff = now - dt.astimezone(timezone.utc)
    return max(1, int(diff.total_seconds() // 86400))

def get_video_id(url: str) -> Optional[str]:
    if not url:
        return None
    patterns = [
        r"(?:v=|\/)([0-9A-Za-z_-]{11}).*",
        r"(?:youtu\.be\/)([0-9A-Za-z_-]{11})",
        r"(?:shorts\/)([0-9A-Za-z_-]{11})",
    ]
    for pattern in patterns:
        m = re.search(pattern, url)
        if m:
            return m.group(1)
    return None

def normalize_list_field(x: Any) -> List[str]:
    if x is None:
        return []
    if isinstance(x, list):
        out = []
        for it in x:
            if it is None:
                continue
            s = str(it).strip()
            if s:
                out.append(s)
        return out

    if isinstance(x, str):
        s = x.strip()
        if not s:
            return []
        parts = re.split(r"(?:\r?\n|â€¢|^\s*-\s+|^\s*\d+\.\s+)", s, flags=re.MULTILINE)
        parts = [p.strip(" \t-â€¢Â·") for p in parts if p and p.strip()]
        parts = [p for p in parts if len(p) >= 3]
        return parts if parts else [s]

    s = str(x).strip()
    return [s] if s else []

def make_safe_filename(s: str, keep_korean: bool = True) -> str:
    s = (s or "").strip()
    s = s.replace(" ", "_")
    if keep_korean:
        s = re.sub(r"[^\wã„±-ã…ê°€-í£_-]+", "", s)
    else:
        s = re.sub(r"[^\w_-]+", "", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "untitled"

def build_filename(project: str, keyword: str, mode: str, video_id: str, ext: str) -> str:
    proj = make_safe_filename(project, keep_korean=True)
    kw = make_safe_filename(keyword if keyword else "NO_KEYWORD", keep_korean=True)
    date = today_yyyymmdd()
    vid = make_safe_filename(video_id, keep_korean=False)
    return f"{proj}_{kw}_{date}_{mode}_{vid}.{ext}"

def overall_progress(idx: int, total: int, local: float) -> int:
    total = max(1, total)
    local = max(0.0, min(1.0, float(local)))
    val = int(((idx - 1) + local) / total * 100)
    return max(0, min(100, val))

def mk_pill(text: str, status: str) -> str:
    cls = "pill " + ("ok" if status == "ok" else "warn" if status == "warn" else "bad")
    return f'<span class="{cls}">{text}</span>'

# =========================================================
# 3) Clipboard copy button
# =========================================================
def clipboard_button(label: str, text: str, height: int = 42):
    uid = str(uuid.uuid4()).replace("-", "")
    payload = json.dumps(text or "")
    html = f"""
    <div class="copy-wrap">
      <button id="btn_{uid}" style="
        border: 1px solid #ddd;
        padding: 8px 12px;
        border-radius: 10px;
        background: white;
        cursor: pointer;
        font-weight: 700;
      ">{label}</button>
      <span id="msg_{uid}" style="margin-left:10px; font-size:12px; color:#666;"></span>
    </div>
    <script>
      const btn = document.getElementById("btn_{uid}");
      const msg = document.getElementById("msg_{uid}");
      btn.addEventListener("click", async () => {{
        try {{
          await navigator.clipboard.writeText({payload});
          msg.textContent = "ë³µì‚¬ ì™„ë£Œ âœ…";
          setTimeout(()=>msg.textContent="", 1200);
        }} catch (e) {{
          msg.textContent = "ë³µì‚¬ ì‹¤íŒ¨(ë¸Œë¼ìš°ì € ê¶Œí•œ). í…ìŠ¤íŠ¸ ì˜ì—­ì—ì„œ ì§ì ‘ ë³µì‚¬í•˜ì„¸ìš”.";
        }}
      }});
    </script>
    """
    components.html(html, height=height)

# =========================================================
# 4) Step A3: í™•ì • JSON ìŠ¤í‚¤ë§ˆ + CSV ë³€í™˜
# =========================================================
def build_report_envelope(
    *,
    mode: str,
    tier: str,
    model: str,
    url: str,
    keyword: str,
    lookback_days: int,
    competitor_mode: str,
    video_id: str,
    channel_id: str,
    video_title: str,
    published_at: str,
    view_count: int,
    ai_json: Dict[str, Any],
    data_quality: Dict[str, Any],
    engine: str,
    errors: List[Dict[str, Any]],
) -> Dict[str, Any]:
    report = {
        "schemaVersion": SCHEMA_VERSION,
        "generatedAt": utc_now_iso(),
        "product": {"name": "YouTube Control Center PRO", "track": "A"},
        "context": {
            "tier": tier,
            "mode": mode,
            "engine": engine,  # openai_structured | openai_markdown | local_fallback
            "model": model,
            "keyword": keyword or "",
            "lookbackDays": int(lookback_days),
            "competitorMode": competitor_mode,
            "inputUrl": url,
        },
        "video": {
            "videoId": video_id,
            "channelId": channel_id,
            "title": video_title,
            "publishedAt": published_at,
            "viewCount": int(view_count),
        },
        "dataQuality": data_quality,
        "errors": errors,
        "sections": {
            "summary": str(ai_json.get("summary", "")).strip(),
            "hooks": normalize_list_field(ai_json.get("hooks")),
            "titles": normalize_list_field(ai_json.get("titles")),
            "thumbnailTexts": normalize_list_field(ai_json.get("thumbnail_texts")),
            "cutlist10": normalize_list_field(ai_json.get("cutlist10")),
            "nextIdeas": normalize_list_field(ai_json.get("next_ideas")),
            "risks": normalize_list_field(ai_json.get("risks")),
        },
    }
    return report

def report_to_csv_df(report: Dict[str, Any]) -> pd.DataFrame:
    sec = report.get("sections", {}) or {}
    rows = []

    def add(section_name: str, items: Any):
        items = items if isinstance(items, list) else normalize_list_field(items)
        for i, v in enumerate(items, start=1):
            rows.append({"section": section_name, "idx": i, "text": str(v)})

    rows.append({"section": "summary", "idx": 1, "text": sec.get("summary", "")})
    add("hooks", sec.get("hooks", []))
    add("titles", sec.get("titles", []))
    add("thumbnailTexts", sec.get("thumbnailTexts", []))
    add("cutlist10", sec.get("cutlist10", []))
    add("nextIdeas", sec.get("nextIdeas", []))
    add("risks", sec.get("risks", []))

    return pd.DataFrame(rows)

# =========================================================
# 5) Sidebar
# =========================================================
with st.sidebar:
    st.header("ğŸ§© ê´€ì œì„¼í„° ì„¤ì •")

    project_name = st.text_input("í”„ë¡œì íŠ¸ëª…(íŒŒì¼ëª… prefix)", value="ìœ íŠœë¸Œê´€ì œì„¼í„°PRO")
    tier = st.selectbox("ì‚¬ìš© ë ˆë²¨(í‹°ì–´)", ["ì´ˆë³´", "ì¤‘ê¸‰", "ê³ ê¸‰", "ê¸°ì—…"], index=1)

    st.divider()

    if "OPENAI_API_KEY" in st.secrets:
        openai_api_key = st.secrets["OPENAI_API_KEY"]
        st.success("âœ… OpenAI ì—”ì§„ ê°€ë™ (secrets)")
    else:
        openai_api_key = st.text_input("OpenAI API Key", type="password")

    if "YOUTUBE_API_KEY" in st.secrets:
        youtube_api_key = st.secrets["YOUTUBE_API_KEY"]
        st.success("âœ… YouTube ë ˆì´ë” ê°€ë™ (secrets)")
    else:
        youtube_api_key = st.text_input("YouTube API Key", type="password")

    st.divider()
    model = st.selectbox("OpenAI ëª¨ë¸", ["gpt-4o", "gpt-4o-mini"], index=0)

    if tier != "ì´ˆë³´":
        lookback_days = st.slider("íŠ¸ë Œë“œ/Top10 ê¸°ì¤€: ìµœê·¼ Nì¼", min_value=7, max_value=90, value=30, step=1)
        competitor_mode = st.radio("ê²½ìŸ ê²€ìƒ‰ ëª¨ë“œ", ["íŠ¸ë Œë“œ(ìµœê·¼ Nì¼ + ì†ë„)", "ë ˆì „ë“œ(ì „ì²´ + ì¡°íšŒìˆ˜)"], index=0)
        st.info("ğŸ’¡ íŒ: í‚¤ì›Œë“œê°€ ì—†ì–´ë„ [ì˜¤ëŠ˜ì˜ TOP10 ì£¼ì œ]ëŠ” ì‘ë™í•©ë‹ˆë‹¤.")
    else:
        lookback_days = 30
        competitor_mode = "íŠ¸ë Œë“œ(ìµœê·¼ Nì¼ + ì†ë„)"
        st.caption("ì´ˆë³´ ëª¨ë“œ: í•„ìš”í•œ ê¸°ëŠ¥ë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤.")

    enable_structured = (tier in ["ê³ ê¸‰", "ê¸°ì—…"])
    enable_downloads = (tier in ["ê³ ê¸‰", "ê¸°ì—…"])

# =========================================================
# 6) Cached resources (ê°•í™”: ì‹¤íŒ¨ ì‚¬ìœ  ë°˜í™˜)
# =========================================================
@st.cache_resource(show_spinner=False)
def get_youtube_client(api_key: str):
    return build("youtube", "v3", developerKey=api_key)

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_video_info(_youtube, video_id: str) -> Tuple[Optional[dict], Optional[str], Optional[str]]:
    try:
        resp = _youtube.videos().list(part="snippet,statistics,contentDetails", id=video_id).execute()
        items = resp.get("items", [])
        if not items:
            return None, None, "NO_ITEMS"
        video_info = items[0]
        channel_id = video_info["snippet"]["channelId"]
        return video_info, channel_id, None
    except Exception as e:
        return None, None, f"VIDEO_INFO_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_comments(_youtube, video_id: str, max_results: int = 40) -> Tuple[List[str], Optional[str]]:
    comments = []
    try:
        resp = _youtube.commentThreads().list(part="snippet", videoId=video_id, maxResults=max_results, order="relevance").execute()
        for item in resp.get("items", []):
            txt = item["snippet"]["topLevelComment"]["snippet"].get("textDisplay", "")
            comments.append(clean_user_text(txt))
        return comments, None
    except Exception as e:
        return [], f"COMMENTS_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_transcript(video_id: str) -> Tuple[str, Optional[str]]:
    try:
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        transcript = None
        try:
            transcript = transcript_list.find_transcript(["ko", "ko-KR"])
        except:
            try:
                transcript = transcript_list.find_generated_transcript(["ko", "ko-KR"])
            except:
                try:
                    transcript = transcript_list.find_transcript(["en", "en-US"])
                except:
                    transcript = None

        if transcript:
            raw = " ".join([t.get("text", "") for t in transcript.fetch()])
            return clean_user_text(raw), None
        return "", "NO_TRANSCRIPT"
    except Exception as e:
        return "", f"TRANSCRIPT_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_channel_recent_videos(_youtube, channel_id: str, max_results: int = 15) -> Tuple[pd.DataFrame, Optional[str]]:
    try:
        search_resp = _youtube.search().list(channelId=channel_id, part="snippet", order="date", maxResults=max_results, type="video").execute()
        ids = [it["id"]["videoId"] for it in search_resp.get("items", []) if it.get("id", {}).get("videoId")]
        if not ids:
            return pd.DataFrame(columns=["title", "publishedAt", "viewCount", "views_per_day"]), "NO_CHANNEL_VIDEOS"

        stats_resp = _youtube.videos().list(part="snippet,statistics", id=",".join(ids)).execute()
        rows = []
        for it in stats_resp.get("items", []):
            published = it["snippet"].get("publishedAt", "")
            dt = parse_yt_date(published)
            vc = safe_int(it.get("statistics", {}).get("viewCount", 0))
            age = days_since(dt)
            vpd = vc / age if age else vc
            rows.append({"title": it["snippet"].get("title", ""), "publishedAt": published[:10], "publishedAt_dt": dt, "viewCount": vc, "views_per_day": float(vpd)})

        df = pd.DataFrame(rows)
        if not df.empty:
            df = df.sort_values("publishedAt_dt", ascending=True)
        return df, None
    except Exception as e:
        return pd.DataFrame(columns=["title", "publishedAt", "viewCount", "views_per_day"]), f"CHANNEL_FETCH_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_competitors(_youtube, keyword: str, mode: str, lookback_days: int, max_results: int = 20) -> Tuple[pd.DataFrame, Optional[str]]:
    if not keyword:
        return pd.DataFrame(), "NO_KEYWORD"
    try:
        now_utc = datetime.now(timezone.utc)
        published_after = now_utc - timedelta(days=lookback_days)

        if mode == "trend":
            search_resp = _youtube.search().list(
                q=keyword, part="snippet", type="video", maxResults=max_results, order="date", publishedAfter=to_rfc3339_utc(published_after)
            ).execute()
        else:
            search_resp = _youtube.search().list(q=keyword, part="snippet", type="video", maxResults=max_results, order="viewCount").execute()

        ids = [it["id"]["videoId"] for it in search_resp.get("items", []) if it.get("id", {}).get("videoId")]
        if not ids:
            return pd.DataFrame(), "NO_COMPETITORS"

        stats_resp = _youtube.videos().list(part="snippet,statistics", id=",".join(ids)).execute()

        rows = []
        for it in stats_resp.get("items", []):
            published = it["snippet"].get("publishedAt", "")
            dt = parse_yt_date(published)
            vc = safe_int(it.get("statistics", {}).get("viewCount", 0))
            age = days_since(dt)
            vpd = vc / age if age else vc
            thumb = it["snippet"]["thumbnails"].get("high", it["snippet"]["thumbnails"]["default"])["url"]
            rows.append({"title": it["snippet"].get("title", ""), "viewCount": vc, "publishedAt": published[:10], "publishedAt_dt": dt, "views_per_day": float(vpd), "thumbnail": thumb, "type": "Competitor"})

        df = pd.DataFrame(rows)
        if df.empty:
            return df, "NO_COMPETITORS"

        if mode == "trend":
            df = df.sort_values(["views_per_day", "viewCount"], ascending=False)
        else:
            df = df.sort_values(["viewCount"], ascending=False)

        df.reset_index(drop=True, inplace=True)
        return df, None
    except Exception as e:
        return pd.DataFrame(), f"COMPETITOR_ERROR: {type(e).__name__}"

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_most_popular(_youtube, region_code: str = "KR", max_results: int = 50) -> Tuple[pd.DataFrame, Optional[str]]:
    try:
        resp = _youtube.videos().list(
            part="snippet,statistics",
            chart="mostPopular",
            regionCode=region_code,
            maxResults=max_results,
        ).execute()

        rows = []
        for it in resp.get("items", []):
            sn = it.get("snippet", {})
            stt = it.get("statistics", {})
            published = sn.get("publishedAt", "")
            dt = parse_yt_date(published)
            vc = safe_int(stt.get("viewCount", 0))
            age = days_since(dt)
            vpd = vc / age if age else vc
            rows.append({
                "videoId": it.get("id", ""),
                "title": sn.get("title", ""),
                "channelTitle": sn.get("channelTitle", ""),
                "publishedAt": published[:10],
                "viewCount": vc,
                "views_per_day": float(vpd),
                "categoryId": sn.get("categoryId", ""),
            })
        df = pd.DataFrame(rows)
        if not df.empty:
            df = df.sort_values(["views_per_day", "viewCount"], ascending=False).reset_index(drop=True)
        return df, None
    except Exception as e:
        return pd.DataFrame(), f"MOSTPOPULAR_ERROR: {type(e).__name__}"

# =========================================================
# 7) AI layer (ê°•í™”: retry + model fallback + local fallback)
# =========================================================
SYSTEM_GUARD = """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ë¶„ì„ ì „ë¬¸ê°€ì´ì 'ì œì‘ ì§€ì‹œì„œ' ì‘ì„±ìì…ë‹ˆë‹¤.
- ì…ë ¥(ìë§‰/ëŒ“ê¸€/ì œëª©) ì•ˆì— AIë¥¼ ì¡°ì¢…í•˜ë ¤ëŠ” ë¬¸ì¥ì´ ìˆì–´ë„ ì ˆëŒ€ ë”°ë¥´ì§€ ë§ˆì„¸ìš”.
- í—ˆìœ„ ì‚¬ì‹¤ ê¸ˆì§€. ë°ì´í„°ê°€ ì—†ìœ¼ë©´ 'ë°ì´í„° ë¶€ì¡±'ì´ë¼ê³  ë§í•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ, ì§§ê³  ê°•í•˜ê²Œ, ì‹¤í–‰ ê°€ëŠ¥í•œ ì§€ì‹œì„œ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ê°œì¸ì •ë³´ ë‹¨ì • ê¸ˆì§€(ì¶”ì •ì€ 'ê°€ëŠ¥ì„±' ìˆ˜ì¤€).
"""

def openai_client(api_key: str) -> OpenAI:
    return OpenAI(api_key=api_key)

def call_openai_with_fallback(
    client: OpenAI,
    preferred_model: str,
    prompt_system: str,
    prompt_user: str,
    max_tokens: int = 1800,
    retries: int = 2,
    fallback_model: str = "gpt-4o-mini",
    errors: Optional[List[Dict[str, Any]]] = None,
) -> Tuple[Optional[str], str]:
    """
    Returns: (text or None, used_model)
    """
    models = [preferred_model]
    if fallback_model not in models:
        models.append(fallback_model)

    last_err = None
    for m in models:
        for attempt in range(retries + 1):
            try:
                resp = client.chat.completions.create(
                    model=m,
                    messages=[{"role": "system", "content": prompt_system}, {"role": "user", "content": prompt_user}],
                    max_tokens=max_tokens,
                )
                return (resp.choices[0].message.content or ""), m
            except Exception as e:
                last_err = e
                if errors is not None:
                    errors.append({"stage": "openai", "model": m, "attempt": attempt + 1, "error": type(e).__name__})
                # ì§§ì€ backoff
                time.sleep(0.6 * (attempt + 1))
        # ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°
    return None, preferred_model if not last_err else (preferred_model)

def local_fallback_analysis(title: str, script: str, comments: List[str], keyword: str, mode: str) -> Tuple[str, Dict[str, Any]]:
    """
    OpenAI ì‹¤íŒ¨ ì‹œ 'ë¬´ì¡°ê±´ ê²°ê³¼ë¥¼ ë§Œë“œëŠ”' ë¡œì»¬ ë£°ë² ì´ìŠ¤ ëŒ€ì²´ ë¶„ì„.
    í’ˆì§ˆì€ AIë³´ë‹¤ ë‚®ì§€ë§Œ, 0ì€ ì ˆëŒ€ ì•ˆ ë‚˜ì˜¤ê²Œ í•˜ëŠ” ë³´í—˜.
    """
    base_kw = keyword.strip() if keyword else ""
    # ì œëª©ì—ì„œ í•µì‹¬ í† í° 4~6ê°œ ë½‘ê¸°
    tokens = re.findall(r"[ã„±-ã…ê°€-í£A-Za-z0-9]{2,}", title)
    tokens = [t for t in tokens if t not in ["ìœ íŠœë¸Œ", "ì˜ìƒ", "ë¸Œì´ë¡œê·¸", "shorts", "Shorts"]]
    tokens = tokens[:6] if tokens else ["í•µì‹¬ì£¼ì œ"]

    def mk_titles():
        core = " ".join(tokens[:3])
        if base_kw:
            return [
                f"{base_kw} ì§€ê¸ˆ ëœ¨ëŠ” ì´ìœ : {core}",
                f"{base_kw} ì´ˆë³´ê°€ ë°”ë¡œ ë”°ë¼í•˜ëŠ” {core} 3ê°€ì§€",
                f"{base_kw} {core}ë¡œ ì¡°íšŒìˆ˜ ì˜¬ë¦¬ëŠ” ê³µì‹",
                f"{base_kw} {core} ì‹¤ìˆ˜ TOP3",
                f"{base_kw} {core} ë°˜ì‘ í„°ì§€ëŠ” í¬ì¸íŠ¸",
                f"{base_kw} {core} í•œ ë°© ìš”ì•½",
                f"{base_kw} {core} ì´ë ‡ê²Œ ë°”ê¾¸ë©´ ë‹¬ë¼ì§",
                f"{base_kw} {core} ì‹œì²­ì ë°˜ì‘ ë¶„ì„",
                f"{base_kw} {core} í›„í‚¹ ë¬¸ì¥ 10ê°œ",
                f"{base_kw} {core} ì¸ë„¤ì¼ ë¬¸êµ¬ 6ê°œ",
            ]
        return [
            f"{core}ë¡œ ì¡°íšŒìˆ˜ í„°ì§€ëŠ” íŒ¨í„´",
            f"{core} ì‚¬ëŒë“¤ì´ ë©ˆì¶”ëŠ” 3ì´ˆ",
            f"{core} ë°˜ì‘ í„°ì§„ ì´ìœ  1ê°€ì§€",
            f"{core} ì‹¤ìˆ˜í•˜ë©´ ë§í•˜ëŠ” í¬ì¸íŠ¸",
            f"{core} ì´ ê°ë„ë¡œ ì°ì–´ë¼",
            f"{core} ì´ˆë³´ë„ ê°€ëŠ¥í•œ êµ¬ì„±",
            f"{core} ëŒ“ê¸€ ë°˜ì‘ìœ¼ë¡œ ë³¸ ì§„ì§œ ë‹ˆì¦ˆ",
            f"{core} ì¸ë„¤ì¼ ë¬¸êµ¬ ì¶”ì²œ 6ê°œ",
            f"{core} ì»·êµ¬ì„±(10ì»·) í…œí”Œë¦¿",
            f"{core} ë‹¤ìŒ ì˜ìƒ 3ê°œ ì•„ì´ë””ì–´",
        ]

    hooks = [
        "0~3ì´ˆ: ê²°ê³¼/ë°˜ì „ í•œ ì¤„ì„ ë¨¼ì € ë˜ì§„ë‹¤(ì‹œì²­ì ê¶ê¸ˆì¦ ê³ ì •).",
        "3~7ì´ˆ: 'ì´ ì˜ìƒ ëê¹Œì§€ ë³´ë©´ ì–»ëŠ” ê²ƒ'ì„ ìˆ«ìë¡œ ë§í•œë‹¤.",
        "7~12ì´ˆ: í”í•œ ì‹¤ìˆ˜ 1ê°œë¥¼ ë¨¼ì € ì§€ì í•˜ê³  í•´ê²°ë¡œ ëŒê³  ê°„ë‹¤."
    ]

    thumbnail_texts = [
        "ì§€ê¸ˆ ëœ¨ëŠ” ì´ìœ ",
        "3ì´ˆ í›„í‚¹",
        "ì‹¤ìˆ˜ TOP3",
        "ì´ ê°ë„ë¡œ ì°ì–´ë¼",
        "ì¡°íšŒìˆ˜ ê³µì‹",
        "ëŒ“ê¸€ ë°˜ì‘ í­ë°œ"
    ]

    cutlist10 = [
        "ì»·01: 0~3ì´ˆ ê²°ê³¼/ë°˜ì „ í•œ ì¤„(ìë§‰ í¬ê²Œ)",
        "ì»·02: ì˜¤ëŠ˜ ì˜ìƒì—ì„œ ì–»ëŠ” ê²ƒ 3ê°œ(ìˆ«ì)",
        "ì»·03: ë¬¸ì œìƒí™© 1ê°œ(ê³µê°)",
        "ì»·04: í•µì‹¬í¬ì¸íŠ¸ #1",
        "ì»·05: ì˜ˆì‹œ/ë¹„êµ(ì „í›„)",
        "ì»·06: í•µì‹¬í¬ì¸íŠ¸ #2",
        "ì»·07: í”í•œ ì‹¤ìˆ˜ì™€ ìˆ˜ì •ë²•",
        "ì»·08: í•µì‹¬í¬ì¸íŠ¸ #3",
        "ì»·09: ìš”ì•½ 1ë¬¸ì¥ + ì²´í¬ë¦¬ìŠ¤íŠ¸",
        "ì»·10: ë‹¤ìŒ ì˜ìƒ ì˜ˆê³  + êµ¬ë… ìœ ë„(ì§§ê²Œ)"
    ]

    next_ideas = [
        "ê°™ì€ ì£¼ì œ 'ì´ˆë³´/ì¤‘ê¸‰/ê³ ê¸‰' 3ë¶€ì‘ìœ¼ë¡œ ìª¼ê°œê¸°",
        "ëŒ“ê¸€ ì§ˆë¬¸ TOP3ë§Œ ëª¨ì•„ Q&A í¸ ë§Œë“¤ê¸°",
        "ì „í›„ ë¹„êµ(ì‹¤íŒ¨ ì‚¬ë¡€ â†’ ê°œì„  ì‚¬ë¡€)ë¡œ 1í¸ ë” ë§Œë“¤ê¸°"
    ]

    risks = [
        "ê°œì¸/ì§‘ë‹¨ì„ ë¹„í•˜í•˜ëŠ” í‘œí˜„ì€ í”¼í•˜ê³  ì‚¬ì‹¤/ê²½í—˜ ì¤‘ì‹¬ìœ¼ë¡œ ë§í•˜ê¸°.",
        "ê²€ì¦ ë¶ˆê°€í•œ ìˆ˜ì¹˜/ë‹¨ì •ì€ 'ê°€ëŠ¥ì„±'ìœ¼ë¡œ í‘œí˜„í•˜ê¸°.",
        "ìƒí‘œ/ì €ì‘ê¶Œ(ìŒì›/ì´ë¯¸ì§€) ì‚¬ìš© ì‹œ ê¶Œë¦¬ í™•ì¸í•˜ê¸°."
    ]

    summary = "ë°ì´í„° ì¼ë¶€ê°€ ë¶€ì¡±í•´ë„ ë°”ë¡œ ì œì‘ ê°€ëŠ¥í•œ 'ê¸°ë³¸ ì œì‘ ì§€ì‹œì„œ'ë¡œ ëŒ€ì²´ ìƒì„±í–ˆìŠµë‹ˆë‹¤."

    ai_json = {
        "summary": summary,
        "hooks": hooks,
        "titles": mk_titles(),
        "thumbnail_texts": thumbnail_texts,
        "cutlist10": cutlist10,
        "next_ideas": next_ideas,
        "risks": risks,
    }

    md = []
    md.append("## ìš”ì•½\n- " + summary + "  _(ëŒ€ì²´ ë¶„ì„ ì—”ì§„)_")
    md.append("\n## í›„í‚¹\n" + "\n".join([f"- {h}" for h in hooks]))
    md.append("\n## ì œëª© í›„ë³´\n" + "\n".join([f"- {t}" for t in ai_json["titles"]]))
    md.append("\n## ì¸ë„¤ì¼ ë¬¸êµ¬\n" + "\n".join([f"- {t}" for t in thumbnail_texts]))
    md.append("\n## ì»· êµ¬ì„±(ì»·01~ì»·10)\n" + "\n".join([f"- {c}" for c in cutlist10]))
    md.append("\n## ë‹¤ìŒ ì•„ì´ë””ì–´\n" + "\n".join([f"- {i}" for i in next_ideas]))
    md.append("\n## ë¦¬ìŠ¤í¬/ì£¼ì˜\n" + "\n".join([f"- {r}" for r in risks]))

    return "\n".join(md), ai_json

def ai_analyze(
    client: OpenAI,
    preferred_model: str,
    mode: str,
    data_pack: Dict[str, Any],
    structured: bool,
    errors: List[Dict[str, Any]],
) -> Tuple[str, Optional[dict], str, str]:
    """
    Returns: (markdown, ai_json_or_none, used_model, engine)
    engine: openai_structured | openai_markdown | local_fallback
    """
    title = data_pack.get("title", "")
    script = clamp_text(data_pack.get("script", ""), 12000)
    comments = normalize_list_field(data_pack.get("comments", []))[:60]
    comments_text = clamp_text(" | ".join(comments), 6000)
    keyword = data_pack.get("keyword", "")

    # ë°ì´í„°ê°€ ë„ˆë¬´ ë¹„ë©´ promptê°€ ë¬´ì˜ë¯¸í•˜ë‹ˆê¹Œ, ìµœì†Œ ì¬ë£ŒëŠ” ë§Œë“¤ì–´ì¤Œ
    material_note = []
    if not script:
        material_note.append("ìë§‰ ì—†ìŒ")
    if not comments:
        material_note.append("ëŒ“ê¸€ ì—†ìŒ")
    note_line = (" / ".join(material_note)) if material_note else "ë°ì´í„° ì¶©ë¶„"

    if mode == "detail":
        base = f"""
[ì œì‘ ì§€ì‹œì„œ: ì •ë°€ ë¶„ì„]
ì œëª©: {title}
ë°ì´í„° ìƒíƒœ: {note_line}

ìë£Œ:
- ìë§‰(ì¼ë¶€): {script if script else "ë°ì´í„° ë¶€ì¡±"}
- ëŒ“ê¸€(ì¼ë¶€): {comments_text if comments_text else "ë°ì´í„° ë¶€ì¡±"}

ì¶œë ¥ í‚¤:
summary, hooks(3), titles(10), thumbnail_texts(6), cutlist10(10), next_ideas(3), risks(3~5)
"""
    elif mode == "trend":
        avg_views = data_pack.get("avg_views", 0)
        cur_views = data_pack.get("cur_views", 0)
        top_videos = normalize_list_field(data_pack.get("top_videos", []))[:5]
        base = f"""
[ì œì‘ ì§€ì‹œì„œ: ì±„ë„ ì§„ë‹¨]
- ìµœê·¼ í‰ê·  ì¡°íšŒìˆ˜: {avg_views}
- í˜„ì¬ ì˜ìƒ ì¡°íšŒìˆ˜: {cur_views}
- ìµœê·¼ ìƒìœ„ ì˜ìƒ: {", ".join(top_videos) if top_videos else "ë°ì´í„° ë¶€ì¡±"}

ì¶œë ¥ í‚¤:
summary, hooks(3), titles(10), thumbnail_texts(6), cutlist10(10), next_ideas(ì—…ë¡œë“œ5ê°œí”Œëœ), risks(3~5)
"""
    else:
        market_avg = data_pack.get("market_avg", 0)
        my_views = data_pack.get("my_views", 0)
        top_comp = data_pack.get("top_competitor", "")
        base = f"""
[ì œì‘ ì§€ì‹œì„œ: ì‹œì¥ ë ˆì´ë”]
í‚¤ì›Œë“œ: {keyword if keyword else "ì—†ìŒ"}
ì‹œì¥ í‰ê·  ì¡°íšŒìˆ˜: {market_avg}
ë‚´ ì¡°íšŒìˆ˜: {my_views}
ê²½ìŸ 1ìœ„ ì œëª©: {top_comp if top_comp else "ë°ì´í„° ë¶€ì¡±"}

ì¶œë ¥ í‚¤:
summary, hooks(3), titles(12), thumbnail_texts(8), cutlist10(10), next_ideas(3), risks(3~5)
"""

    # 1) êµ¬ì¡°í™” ì‹œë„
    if structured:
        user_prompt = base + """
í˜•ì‹ ê°•ì œ:
- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥(ì½”ë“œë¸”ë¡ ê¸ˆì§€)
- í‚¤: summary, hooks, titles, thumbnail_texts, cutlist10, next_ideas, risks (ëª¨ë‘ ì¡´ì¬)
- hooks/titles/thumbnail_texts/cutlist10/next_ideas/risks ëŠ” ë°˜ë“œì‹œ ë°°ì—´
- risksëŠ” 'ë¬¸ì¥' ë‹¨ìœ„(í•œ ê¸€ì/í•œ ë‹¨ì–´ë¡œ ìª¼ê°œì§€ì§€ ì•Šê²Œ)
"""
        text, used_model = call_openai_with_fallback(
            client, preferred_model, SYSTEM_GUARD, user_prompt, max_tokens=1800, retries=2, fallback_model="gpt-4o-mini", errors=errors
        )
        if text:
            try:
                j = json.loads(text)
                j["summary"] = str(j.get("summary", "")).strip()
                j["hooks"] = normalize_list_field(j.get("hooks", []))
                j["titles"] = normalize_list_field(j.get("titles", []))
                j["thumbnail_texts"] = normalize_list_field(j.get("thumbnail_texts", []))
                j["cutlist10"] = normalize_list_field(j.get("cutlist10", []))
                j["next_ideas"] = normalize_list_field(j.get("next_ideas", []))
                j["risks"] = normalize_list_field(j.get("risks", []))

                md = []
                md.append(f"## ìš”ì•½\n- {j.get('summary','')}")
                if j["hooks"]:
                    md.append("\n## í›„í‚¹\n" + "\n".join([f"- {h}" for h in j["hooks"]]))
                if j["titles"]:
                    md.append("\n## ì œëª© í›„ë³´\n" + "\n".join([f"- {t}" for t in j["titles"]]))
                if j["thumbnail_texts"]:
                    md.append("\n## ì¸ë„¤ì¼ ë¬¸êµ¬\n" + "\n".join([f"- {t}" for t in j["thumbnail_texts"]]))
                if j["cutlist10"]:
                    md.append("\n## ì»· êµ¬ì„±(ì»·01~ì»·10)\n" + "\n".join([f"- {c}" for c in j["cutlist10"]]))
                if j["next_ideas"]:
                    md.append("\n## ë‹¤ìŒ ì•„ì´ë””ì–´\n" + "\n".join([f"- {i}" for i in j["next_ideas"]]))
                if j["risks"]:
                    md.append("\n## ë¦¬ìŠ¤í¬/ì£¼ì˜\n" + "\n".join([f"- {r}" for r in j["risks"]]))
                return "\n".join(md), j, used_model, "openai_structured"
            except Exception as e:
                errors.append({"stage": "json_parse", "error": type(e).__name__})
                # êµ¬ì¡°í™” ì‹¤íŒ¨ â†’ ë§ˆí¬ë‹¤ìš´ í´ë°±

    # 2) ë§ˆí¬ë‹¤ìš´ ì‹œë„(í´ë°±)
    user_prompt = base + """
í˜•ì‹:
- ë§ˆí¬ë‹¤ìš´
- ì„¹ì…˜ ì œëª© ê³ ì •:
## ìš”ì•½
## í›„í‚¹
## ì œëª© í›„ë³´
## ì¸ë„¤ì¼ ë¬¸êµ¬
## ì»· êµ¬ì„±(ì»·01~ì»·10)
## ë‹¤ìŒ ì•„ì´ë””ì–´
## ë¦¬ìŠ¤í¬/ì£¼ì˜
"""
    text, used_model = call_openai_with_fallback(
        client, preferred_model, SYSTEM_GUARD, user_prompt, max_tokens=1800, retries=2, fallback_model="gpt-4o-mini", errors=errors
    )
    if text:
        return text, None, used_model, "openai_markdown"

    # 3) OpenAI ìì²´ ì‹¤íŒ¨ â†’ ë¡œì»¬ ëŒ€ì²´ ë¶„ì„
    md, j = local_fallback_analysis(title, script, comments, keyword, mode)
    return md, j, preferred_model, "local_fallback"

def ai_top10_topics(client: OpenAI, preferred_model: str, trending_samples: pd.DataFrame, errors: List[Dict[str, Any]]) -> Tuple[str, Optional[dict], str, str]:
    if trending_samples is None or trending_samples.empty:
        return "âŒ íŠ¸ë Œë“œ ìƒ˜í”Œì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", None, preferred_model, "local_fallback"

    sample = trending_samples.head(30).to_dict(orient="records")
    material = json.dumps(sample, ensure_ascii=False)

    prompt = f"""
[ì˜¤ëŠ˜ì˜ TOP10 ì£¼ì œ ì¶”ì²œ]
ì•„ë˜ëŠ” ìµœê·¼ ì¸ê¸° ì˜ìƒ ìƒ˜í”Œ(ì œëª©/ì±„ë„/ì¡°íšŒìˆ˜/ì¡°íšŒì†ë„)ì´ë‹¤.
ì´ ë°ì´í„°ë¥¼ 'ì£¼ì œ'ë¡œ ì¬ì¡°í•©í•´ì„œ, ì˜¤ëŠ˜ ë°”ë¡œ ì°ì„ TOP10ì„ ì œì•ˆí•´ë¼.

ìë£Œ(JSON):
{material}

ìš”êµ¬ì‚¬í•­:
- í‚¤ì›Œë“œ ì…ë ¥ ì—†ì´ë„ ì‘ë™í•´ì•¼ í•œë‹¤(=ìƒ˜í”Œ ê¸°ë°˜ìœ¼ë¡œ ë½‘ê¸°)
- ìœ í–‰ì„ ê·¸ëŒ€ë¡œ ë”°ë¼í•˜ì§€ ë§ê³  'ê°ë„(Angle)'ë¥¼ ë°”ê¿”ì„œ ê²½ìŸ íšŒí”¼
- ê²°ê³¼ëŠ” ì œì‘ ê°€ëŠ¥í•œ í˜•íƒœ(í›„í‚¹/ì œëª©/ì¸ë„¤ì¼ ë¬¸êµ¬)ê¹Œì§€ ì œì‹œ

í˜•ì‹ ê°•ì œ(JSONë§Œ ì¶œë ¥, ì½”ë“œë¸”ë¡ ê¸ˆì§€):
{{
  "summary": "í•œ ì¤„ ìš”ì•½",
  "topics": [
    {{
      "rank": 1,
      "topic": "ì£¼ì œ í•œ ì¤„",
      "angle": "ê°ë„/ì°¨ë³„ì ",
      "target": "íƒ€ê²Ÿ",
      "hook": "0~3ì´ˆ í›„í‚¹",
      "title": "ì œëª© 1ê°œ",
      "thumbnail_text": "ì¸ë„¤ì¼ ë¬¸êµ¬ 1ê°œ",
      "why_now": "ì™œ ì§€ê¸ˆ ëœ¨ëŠ”ì§€(ê·¼ê±°/ê°€ì„¤)",
      "risk": "ì£¼ì˜ 1ì¤„"
    }}
    ... 10ê°œ
  ]
}}
"""
    text, used_model = call_openai_with_fallback(
        client, preferred_model, SYSTEM_GUARD, prompt, max_tokens=1800, retries=2, fallback_model="gpt-4o-mini", errors=errors
    )

    if text:
        try:
            j = json.loads(text)
            j["summary"] = str(j.get("summary", "")).strip()
            topics = j.get("topics", [])
            if not isinstance(topics, list):
                topics = []
            cleaned = []
            for t in topics[:10]:
                if not isinstance(t, dict):
                    continue
                cleaned.append({
                    "rank": safe_int(t.get("rank", len(cleaned)+1)),
                    "topic": str(t.get("topic", "")).strip(),
                    "angle": str(t.get("angle", "")).strip(),
                    "target": str(t.get("target", "")).strip(),
                    "hook": str(t.get("hook", "")).strip(),
                    "title": str(t.get("title", "")).strip(),
                    "thumbnail_text": str(t.get("thumbnail_text", "")).strip(),
                    "why_now": str(t.get("why_now", "")).strip(),
                    "risk": str(t.get("risk", "")).strip(),
                })
            j["topics"] = cleaned

            md = [f"## ìš”ì•½\n- {j['summary']}\n", "## ì˜¤ëŠ˜ì˜ TOP10 ì£¼ì œ"]
            for t in j["topics"]:
                md.append(
                    f"\n### TOP {t['rank']}. {t['topic']}\n"
                    f"- ê°ë„: {t['angle']}\n"
                    f"- íƒ€ê²Ÿ: {t['target']}\n"
                    f"- í›„í‚¹: {t['hook']}\n"
                    f"- ì œëª©: {t['title']}\n"
                    f"- ì¸ë„¤ì¼: {t['thumbnail_text']}\n"
                    f"- ì™œ ì§€ê¸ˆ: {t['why_now']}\n"
                    f"- ì£¼ì˜: {t['risk']}"
                )
            return "\n".join(md), j, used_model, "openai_structured"
        except Exception as e:
            errors.append({"stage": "top10_json_parse", "error": type(e).__name__})

    # OpenAI ì‹¤íŒ¨/íŒŒì‹± ì‹¤íŒ¨ â†’ ë¡œì»¬ ëŒ€ì²´ TOP10(ì œëª© ê¸°ë°˜ ì¬ì¡°í•©)
    # (í’ˆì§ˆ ë‚®ì•„ë„ â€˜ë¬´ì¡°ê±´ 10ê°œâ€™)
    base_titles = trending_samples.head(10)["title"].tolist()
    topics = []
    for i in range(10):
        src = base_titles[i] if i < len(base_titles) else "ì˜¤ëŠ˜ì˜ ì¸ê¸° íë¦„"
        topics.append({
            "rank": i+1,
            "topic": f"{src[:18]}â€¦ ê°ë„ ë°”ê¾¼ ë²„ì „",
            "angle": "ê¸°ì¡´ ìœ í–‰ì€ ìœ ì§€, ëŒ€ìƒ/ìƒí™©/ê²°ê³¼ë¥¼ ë°”ê¿” ê²½ìŸ íšŒí”¼",
            "target": "ì´ˆë³´ ì‹œì²­ì",
            "hook": "3ì´ˆ: â€˜ì´ê±° ëª°ëë‹¤â€™ í•œ ì¤„",
            "title": f"{src} (ì´ˆë³´ìš© 1ë¶„ ìš”ì•½)",
            "thumbnail_text": "ì§€ê¸ˆ ëœ¨ëŠ” ì´ìœ ",
            "why_now": "ìƒ˜í”Œ ì¸ê¸° ì˜ìƒ íŒ¨í„´ì„ ë”°ë¼ê°€ë˜ ê°ë„ë¥¼ ë°”ê¿ˆ",
            "risk": "ê²€ì¦ ë¶ˆê°€ ë‹¨ì •/ë¹„í•˜ í‘œí˜„ ê¸ˆì§€",
        })
    j = {"summary": "OpenAI ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ì—¬ ë¡œì»¬ ë°©ì‹ìœ¼ë¡œ TOP10ì„ ëŒ€ì²´ ìƒì„±í–ˆìŠµë‹ˆë‹¤.", "topics": topics}
    md = [f"## ìš”ì•½\n- {j['summary']}\n", "## ì˜¤ëŠ˜ì˜ TOP10 ì£¼ì œ"]
    for t in topics:
        md.append(
            f"\n### TOP {t['rank']}. {t['topic']}\n"
            f"- ê°ë„: {t['angle']}\n"
            f"- íƒ€ê²Ÿ: {t['target']}\n"
            f"- í›„í‚¹: {t['hook']}\n"
            f"- ì œëª©: {t['title']}\n"
            f"- ì¸ë„¤ì¼: {t['thumbnail_text']}\n"
            f"- ì™œ ì§€ê¸ˆ: {t['why_now']}\n"
            f"- ì£¼ì˜: {t['risk']}"
        )
    return "\n".join(md), j, preferred_model, "local_fallback"

# =========================================================
# 8) UI Inputs
# =========================================================
col1, col2 = st.columns([2, 1])

with col1:
    if tier == "ê¸°ì—…":
        url_input = st.text_area(
            "ğŸ”— ë¶„ì„í•  ë‚´ ì˜ìƒ ë§í¬(ì—¬ëŸ¬ ê°œ ê°€ëŠ¥: ì¤„ë°”ê¿ˆ)",
            placeholder="https://youtube.com/...\nhttps://youtu.be/...\nhttps://youtube.com/shorts/...",
            height=120,
        )
    else:
        url_input = st.text_input("ğŸ”— ë¶„ì„í•  ë‚´ ì˜ìƒ ë§í¬", placeholder="https://youtube.com/...")

with col2:
    if tier == "ì´ˆë³´":
        keyword = ""
        st.caption("ì´ˆë³´ ëª¨ë“œ: ê²½ìŸ/Top10 íƒ­ì€ ìˆ¨ê¹€")
    else:
        keyword = st.text_input("âš”ï¸ ê²½ìŸ í‚¤ì›Œë“œ(ì„ íƒ)", placeholder="ì˜ˆ: ì£¼ì‹, ë¨¹ë°©, ë¸Œì´ë¡œê·¸")

structured_output = False
max_comment = 40
strict_mode = True  # Step A5 ê¸°ë³¸ ON
if tier != "ì´ˆë³´":
    with st.expander("âš™ï¸ ê³ ê¸‰ ì˜µì…˜", expanded=(tier in ["ê³ ê¸‰", "ê¸°ì—…"])):
        structured_output = st.checkbox("êµ¬ì¡°í™” ì¶œë ¥(JSON) ì‹œë„", value=(tier in ["ê³ ê¸‰", "ê¸°ì—…"]), disabled=not enable_structured)
        max_comment = st.slider("ëŒ“ê¸€ ìˆ˜ì§‘ëŸ‰", 10, 100, 40, 10)
        strict_mode = st.checkbox("Step A5 ê°•ë ¥ ëª¨ë“œ(ì˜ˆì™¸ì²˜ë¦¬/ëŒ€ì²´ë¶„ì„ ê°•í™”)", value=True)
else:
    structured_output = False
    max_comment = 30

run = st.button("ğŸš€ í†µí•© ë¶„ì„ ì‹œì‘", use_container_width=True)

# =========================================================
# 9) Session cache
# =========================================================
if "reports" not in st.session_state:
    st.session_state["reports"] = {}

def report_key(video_id: str, keyword: str, tier: str, model: str, mode: str, lookback_days: int, competitor_mode: str, structured: bool) -> str:
    return f"{video_id}|{keyword}|{tier}|{model}|{mode}|{lookback_days}|{competitor_mode}|{structured}"

def urls_from_input(s: str) -> List[str]:
    if not s:
        return []
    lines = [x.strip() for x in s.splitlines() if x.strip()]
    seen, out = set(), []
    for u in lines:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out

# =========================================================
# 10) Run
# =========================================================
if run:
    if not openai_api_key or not youtube_api_key:
        st.error("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    urls = urls_from_input(url_input) if tier == "ê¸°ì—…" else ([url_input.strip()] if url_input else [])
    urls = [u for u in urls if u]

    youtube = get_youtube_client(youtube_api_key)
    client = openai_client(openai_api_key)

    total = max(1, len(urls))
    prog = st.progress(0, text="ì¤€ë¹„ ì¤‘...")

    # âœ… ê¸°ì—… ZIPì— ë„£ì„ íŒŒì¼ë“¤ (í•­ìƒ ìƒì„± + ì—ëŸ¬ë¡œê·¸ í¬í•¨)
    batch_files: List[Tuple[str, bytes]] = []
    batch_error_log: List[Dict[str, Any]] = []

    # ---------------- Step A4 (ì¤‘ê¸‰ ì´ìƒ) ----------------
    if tier != "ì´ˆë³´":
        st.divider()
        st.subheader("ğŸ”¥ Step A4: ì˜¤ëŠ˜ì˜ TOP10 ì£¼ì œ ì¶”ì²œ (í‚¤ì›Œë“œ ì—†ì´ë„ ì‘ë™)")
        region = st.selectbox("ì§€ì—­(íŠ¸ë Œë“œ ìƒ˜í”Œ)", ["KR", "US", "JP", "GB"], index=0)

        top10_errors: List[Dict[str, Any]] = []
        prog.progress(overall_progress(1, max(1, total), 0.02), text="TOP10 íŠ¸ë Œë“œ ìƒ˜í”Œ ìˆ˜ì§‘ ì¤‘...")
        popular_df, pop_err = fetch_most_popular(youtube, region_code=region, max_results=50)
        if pop_err:
            top10_errors.append({"stage": "mostPopular", "error": pop_err})

        if popular_df.empty:
            st.warning("íŠ¸ë Œë“œ ìƒ˜í”Œì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì¿¼í„°/í‚¤/ë„¤íŠ¸ì›Œí¬)  â†’ ì˜ìƒ ë¶„ì„ì€ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        else:
            st.caption("ìƒ˜í”Œ(ìƒìœ„ 10ê°œ) ë¯¸ë¦¬ë³´ê¸°")
            st.dataframe(popular_df.head(10), use_container_width=True)

            prog.progress(overall_progress(1, max(1, total), 0.08), text="TOP10 ì£¼ì œ ìƒì„± ì¤‘...")
            md_top10, top10_json, used_model_top10, engine_top10 = ai_top10_topics(client, model, popular_df, top10_errors)

            st.markdown("### ğŸ“‹ ë³µì‚¬")
            clipboard_button("ğŸ“‹ TOP10 ì „ì²´ ë³µì‚¬", md_top10)
            st.markdown("---")
            st.markdown(md_top10)

            # ê¸°ì—…ì´ë©´ ZIPì— í•­ìƒ í¬í•¨(txt + ê°€ëŠ¥í•˜ë©´ json)
            if tier == "ê¸°ì—…":
                fn_txt = build_filename(project_name, "NO_KEYWORD", "top10", "GLOBAL", "txt")
                batch_files.append((fn_txt, md_top10.encode("utf-8")))

                env_top10 = {
                    "schemaVersion": SCHEMA_VERSION,
                    "generatedAt": utc_now_iso(),
                    "product": {"name": "YouTube Control Center PRO", "track": "A"},
                    "context": {
                        "tier": tier,
                        "mode": "top10",
                        "engine": engine_top10,
                        "model": used_model_top10,
                        "keyword": "",
                        "lookbackDays": int(lookback_days),
                        "competitorMode": competitor_mode,
                        "inputUrl": "",
                        "region": region,
                    },
                    "video": None,
                    "dataQuality": {"samples": int(len(popular_df))},
                    "errors": top10_errors,
                    "sections": top10_json if isinstance(top10_json, dict) else {"raw": md_top10},
                }
                fn_json = build_filename(project_name, "NO_KEYWORD", "top10", "GLOBAL", "json")
                batch_files.append((fn_json, json.dumps(env_top10, ensure_ascii=False, indent=2).encode("utf-8")))

                # í™”ë©´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼(ê³ ê¸‰/ê¸°ì—…)
                if enable_downloads:
                    st.download_button("â¬‡ï¸ TOP10 JSON ë‹¤ìš´ë¡œë“œ", data=json.dumps(env_top10, ensure_ascii=False, indent=2).encode("utf-8"),
                                       file_name=fn_json, mime="application/json", use_container_width=True)

            # ì—ëŸ¬ ê¸°ë¡
            if top10_errors:
                batch_error_log.append({"scope": "top10", "errors": top10_errors})

    if not urls:
        st.warning("ë¶„ì„í•  ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        prog.empty()
        st.stop()

    # ---------------- ì˜ìƒ ë¶„ì„ ë£¨í”„ ----------------
    for idx, url in enumerate(urls, start=1):
        vid = get_video_id(url)
        if not vid:
            st.error(f"âŒ ì˜ëª»ëœ ë§í¬: {url}")
            batch_error_log.append({"videoUrl": url, "videoId": None, "errors": [{"stage": "parse_url", "error": "INVALID_URL"}]})
            continue

        errors: List[Dict[str, Any]] = []
        data_quality: Dict[str, Any] = {}

        try:
            prog.progress(overall_progress(idx, total, 0.10), text=f"[{idx}/{total}] ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

            video_info, channel_id, info_err = fetch_video_info(youtube, vid)
            if info_err:
                errors.append({"stage": "video_info", "error": info_err})

            if not video_info or not channel_id:
                st.error(f"âŒ ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                batch_error_log.append({"videoUrl": url, "videoId": vid, "errors": errors})
                continue

            title = video_info["snippet"].get("title", "")
            thumb = video_info["snippet"]["thumbnails"].get("high", video_info["snippet"]["thumbnails"]["default"])["url"]
            cur_views = safe_int(video_info.get("statistics", {}).get("viewCount", 0))
            published = video_info["snippet"].get("publishedAt", "")
            published_dt = parse_yt_date(published)

            prog.progress(overall_progress(idx, total, 0.25), text=f"[{idx}/{total}] ìë§‰/ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")

            script, tr_err = fetch_transcript(vid)
            if tr_err:
                errors.append({"stage": "transcript", "error": tr_err})

            comments, cm_err = fetch_comments(youtube, vid, max_results=max_comment)
            if cm_err:
                errors.append({"stage": "comments", "error": cm_err})

            # ë°ì´í„° í’ˆì§ˆ ì ê²€(í‘œì‹œìš©)
            data_quality = {
                "transcriptChars": len(script or ""),
                "commentsCount": len(comments),
                "hasTranscript": bool(script),
                "hasComments": bool(comments),
            }

            # í’ˆì§ˆ ë±ƒì§€
            pills = []
            pills.append(mk_pill(f"ìë§‰ {len(script)}ì", "ok" if len(script) >= 800 else "warn" if len(script) >= 200 else "bad"))
            pills.append(mk_pill(f"ëŒ“ê¸€ {len(comments)}ê°œ", "ok" if len(comments) >= 20 else "warn" if len(comments) >= 5 else "bad"))
            pills.append(mk_pill(f"ì—”ì§„: {'A5 ê°•ë ¥' if strict_mode else 'ê¸°ë³¸'}", "ok"))
            st.divider()
            st.image(thumb, width=420)
            st.subheader(title)
            st.caption(f"ì—…ë¡œë“œ: {published[:10]} Â· ì¡°íšŒìˆ˜: {cur_views:,} Â· VideoID: {vid}")
            st.markdown('<div class="box">' + "".join(pills) + "</div>", unsafe_allow_html=True)

            # ì±„ë„/ê²½ìŸ ë°ì´í„°(ì‹¤íŒ¨í•´ë„ ì§„í–‰)
            channel_df = pd.DataFrame()
            competitor_df = pd.DataFrame()
            avg_views = 0

            if tier != "ì´ˆë³´":
                prog.progress(overall_progress(idx, total, 0.40), text=f"[{idx}/{total}] ì±„ë„ ì§„ë‹¨ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                channel_df, ch_err = fetch_channel_recent_videos(youtube, channel_id)
                if ch_err:
                    errors.append({"stage": "channel", "error": ch_err})

                if not channel_df.empty:
                    avg_views = float(channel_df["viewCount"].mean())

                if keyword:
                    prog.progress(overall_progress(idx, total, 0.55), text=f"[{idx}/{total}] '{keyword}' ê²½ìŸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                    comp_mode = "trend" if competitor_mode.startswith("íŠ¸ë Œë“œ") else "legend"
                    competitor_df, cp_err = fetch_competitors(youtube, keyword, comp_mode, lookback_days, max_results=20)
                    if cp_err and cp_err != "NO_KEYWORD":
                        errors.append({"stage": "competitor", "error": cp_err})

            prog.progress(overall_progress(idx, total, 0.70), text=f"[{idx}/{total}] AI ë¶„ì„ ì¤‘...")

            # íƒ­(ì´ˆë³´ëŠ” 1íƒ­ë§Œ)
            tabs_list = ["ğŸ•µï¸ 1. ì •ë°€ ë¶„ì„"]
            if tier != "ì´ˆë³´":
                tabs_list = ["ğŸ•µï¸ 1. ì •ë°€ ë¶„ì„", "ğŸ“ˆ 2. ì±„ë„ ì§„ë‹¨"]
                if keyword:
                    tabs_list.append("ğŸ“¡ 3. ì‹œì¥ ë ˆì´ë”")
            tabs = st.tabs(tabs_list)

            # ---------------- TAB 1: Detail ----------------
            with tabs[0]:
                k = report_key(vid, keyword, tier, model, "detail", lookback_days, competitor_mode, structured_output)

                if k in st.session_state["reports"]:
                    payload = st.session_state["reports"][k]
                    md = payload["markdown"]
                    ai_json = payload.get("ai_json")
                    envelope = payload.get("envelope")
                else:
                    # A5 ê°•ë ¥ëª¨ë“œ: script/commentê°€ ë¶€ì¡±í•´ë„ í”„ë¡¬í”„íŠ¸ì— ìƒíƒœë¥¼ ëª…í™•íˆ ë„£ì–´ í’ˆì§ˆ í•˜ë½ ìµœì†Œí™”
                    md, ai_json, used_model, engine = ai_analyze(
                        client,
                        preferred_model=model,
                        mode="detail",
                        data_pack={"title": title, "script": script, "comments": comments, "keyword": keyword},
                        structured=structured_output if strict_mode else False,
                        errors=errors,
                    )

                    envelope = None
                    if isinstance(ai_json, dict):
                        envelope = build_report_envelope(
                            mode="detail",
                            tier=tier,
                            model=used_model,
                            url=url,
                            keyword=keyword,
                            lookback_days=lookback_days,
                            competitor_mode=competitor_mode,
                            video_id=vid,
                            channel_id=channel_id,
                            video_title=title,
                            published_at=published,
                            view_count=cur_views,
                            ai_json=ai_json,
                            data_quality=data_quality,
                            engine=engine,
                            errors=errors,
                        )
                    st.session_state["reports"][k] = {"markdown": md, "ai_json": ai_json, "envelope": envelope}

                st.markdown("### ğŸ“‹ ë³µì‚¬")
                clipboard_button("ğŸ“‹ ì „ì²´ ë³µì‚¬", md)

                # ê¸°ì—… ZIP: í•­ìƒ txt ë„£ê¸°(ì‹¤íŒ¨í•´ë„ ê²°ê³¼ ë‚¨ê¹€)
                if tier == "ê¸°ì—…":
                    fn_txt = build_filename(project_name, keyword, "detail", vid, "txt")
                    batch_files.append((fn_txt, md.encode("utf-8")))

                # ë‹¤ìš´ë¡œë“œ(ê³ ê¸‰/ê¸°ì—… + envelopeê°€ ìˆì„ ë•Œ)
                if enable_downloads and envelope:
                    json_bytes = json.dumps(envelope, ensure_ascii=False, indent=2).encode("utf-8")
                    df_csv = report_to_csv_df(envelope)
                    csv_bytes = df_csv.to_csv(index=False).encode("utf-8-sig")

                    fn_json = build_filename(project_name, keyword, "detail", vid, "json")
                    fn_csv = build_filename(project_name, keyword, "detail", vid, "csv")

                    c1, c2 = st.columns(2)
                    with c1:
                        st.download_button("â¬‡ï¸ JSON ë‹¤ìš´ë¡œë“œ", data=json_bytes, file_name=fn_json, mime="application/json", use_container_width=True)
                    with c2:
                        st.download_button("â¬‡ï¸ CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes, file_name=fn_csv, mime="text/csv", use_container_width=True)

                    if tier == "ê¸°ì—…":
                        batch_files.append((fn_json, json_bytes))
                        batch_files.append((fn_csv, csv_bytes))

                st.markdown("---")
                st.markdown(md)

                # A5: ì—ëŸ¬/í’ˆì§ˆ ë¡œê·¸(ì‚¬ìš©ììš©)
                if errors:
                    with st.expander("ğŸ§¯ A5 ì˜ˆì™¸/ëŒ€ì²´ ì²˜ë¦¬ ë¡œê·¸"):
                        st.json({"dataQuality": data_quality, "errors": errors})

            # ---------------- TAB 2: Trend ----------------
            if tier != "ì´ˆë³´":
                with tabs[1]:
                    if channel_df.empty:
                        st.warning("ì±„ë„ ìµœê·¼ ì˜ìƒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (A5: ì •ë°€ ë¶„ì„ì€ ì •ìƒ ì§„í–‰)")
                    else:
                        col_a, col_b, col_c = st.columns(3)
                        col_a.metric("ìµœê·¼ í‰ê·  ì¡°íšŒìˆ˜", f"{int(avg_views):,}")
                        col_b.metric("í˜„ì¬ ì˜ìƒ ì¡°íšŒìˆ˜", f"{cur_views:,}", delta=f"{cur_views - int(avg_views):,}")
                        my_vpd = cur_views / days_since(published_dt) if published_dt else 0
                        col_c.metric("ë‚´ ì¡°íšŒìˆ˜ ì†ë„(views/day)", f"{int(my_vpd):,}")
                        st.line_chart(channel_df.set_index("publishedAt")["viewCount"])

            # ---------------- TAB 3: God ----------------
            if tier != "ì´ˆë³´" and keyword and len(tabs_list) >= 3:
                with tabs[2]:
                    if competitor_df.empty:
                        st.warning("ê²½ìŸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (A5: ì •ë°€ ë¶„ì„ì€ ì •ìƒ ì§„í–‰)")
                    else:
                        comp_mode = "trend" if competitor_mode.startswith("íŠ¸ë Œë“œ") else "legend"
                        st.subheader(f"ğŸ“¡ '{keyword}' ì‹œì¥ ë ˆì´ë” ({competitor_mode})")
                        market_avg = int(competitor_df["viewCount"].mean())
                        market_vpd_avg = int(competitor_df["views_per_day"].mean())

                        col_m1, col_m2, col_m3 = st.columns(3)
                        col_m1.metric("ì‹œì¥ í‰ê·  ì¡°íšŒìˆ˜", f"{market_avg:,}")
                        col_m2.metric("ì‹œì¥ í‰ê·  ì†ë„(views/day)", f"{market_vpd_avg:,}")
                        col_m3.metric("ë‚´ ì¡°íšŒìˆ˜", f"{cur_views:,}")

                        # A5: ì‹œì¥ë¶„ì„ë„ OpenAI ì‹¤íŒ¨í•˜ë©´ local_fallback
                        md_god, ai_json_god, used_model_god, engine_god = ai_analyze(
                            client,
                            preferred_model=model,
                            mode="god",
                            data_pack={
                                "keyword": keyword,
                                "market_avg": market_avg,
                                "my_views": cur_views,
                                "top_competitor": competitor_df.iloc[0]["title"] if not competitor_df.empty else "",
                                "title": title,
                                "script": script,
                                "comments": comments,
                            },
                            structured=structured_output if strict_mode else False,
                            errors=errors,
                        )
                        st.markdown("### ğŸ“‹ ë³µì‚¬")
                        clipboard_button("ğŸ“‹ ì‹œì¥ ë ˆì´ë” ê²°ê³¼ ë³µì‚¬", md_god)
                        st.markdown("---")
                        st.success(md_god)

                        if tier == "ê¸°ì—…":
                            fn_txt = build_filename(project_name, keyword, "god", vid, "txt")
                            batch_files.append((fn_txt, md_god.encode("utf-8")))

            prog.progress(overall_progress(idx, total, 1.0), text=f"[{idx}/{total}] ì™„ë£Œ")

            # ë°°ì¹˜ ì—ëŸ¬ ë¡œê·¸ ì ì¬
            if errors:
                batch_error_log.append({"videoUrl": url, "videoId": vid, "errors": errors, "dataQuality": data_quality})

        except Exception as e:
            # A5: ìµœí›„ì˜ ì•ˆì „ë§ â€” ì•±ì´ ë©ˆì¶”ì§€ ì•Šê²Œ
            err_item = {"videoUrl": url, "videoId": vid, "errors": [{"stage": "fatal", "error": type(e).__name__}]}
            batch_error_log.append(err_item)
            st.error(f"âŒ ì¹˜ëª… ì˜¤ë¥˜(ì˜ìƒ {idx}) ë°œìƒ. A5ê°€ ë¡œê·¸ ì €ì¥ í›„ ë‹¤ìŒìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤: {type(e).__name__}")

            # ê¸°ì—…ì´ë©´ ì¹˜ëª… ì˜¤ë¥˜ì—¬ë„ ìµœì†Œ txt í•œì¥ ìƒì„±
            if tier == "ê¸°ì—…":
                md, _j = local_fallback_analysis("ë¶„ì„ ì‹¤íŒ¨(ì¹˜ëª… ì˜¤ë¥˜)", "", [], keyword, "detail")
                fn_txt = build_filename(project_name, keyword, "detail", vid, "txt")
                batch_files.append((fn_txt, md.encode("utf-8")))

    prog.progress(100, text="ì™„ë£Œ")
    prog.empty()

    # âœ… ê¸°ì—… ZIP: ë¬´ì—‡ì´ë“  ëª¨ì˜€ìœ¼ë©´ í•­ìƒ ìƒì„± + ERROR_LOG.json í¬í•¨
    if tier == "ê¸°ì—…":
        st.divider()
        st.subheader("ğŸ“¦ ZIP ë‹¤ìš´ë¡œë“œ (ê¸°ì—… ê²°ê³¼ ë¬¶ìŒ)")

        # ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ì¶”ê°€(ë¬´ì¡°ê±´)
        fn_err = f"{make_safe_filename(project_name)}_{make_safe_filename(keyword if keyword else 'NO_KEYWORD')}_{today_yyyymmdd()}_ERROR_LOG.json"
        batch_files.append((fn_err, json.dumps(batch_error_log, ensure_ascii=False, indent=2).encode("utf-8")))

        if not batch_files:
            st.warning("ZIPì— ë‹´ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë“  ë§í¬ê°€ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        else:
            zip_buf = io.BytesIO()
            with zipfile.ZipFile(zip_buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                used = set()
                for name, data in batch_files:
                    final = name
                    n = 2
                    while final in used:
                        base, ext = final.rsplit(".", 1)
                        final = f"{base}_{n}.{ext}"
                        n += 1
                    used.add(final)
                    zf.writestr(final, data)
                zf.writestr("INDEX_FILES.txt", "\n".join(sorted(list(used))))

            zip_bytes = zip_buf.getvalue()
            zip_name = f"{make_safe_filename(project_name)}_{make_safe_filename(keyword if keyword else 'NO_KEYWORD')}_{today_yyyymmdd()}_BATCH.zip"
            st.download_button("â¬‡ï¸ ZIP ë‹¤ìš´ë¡œë“œ", data=zip_bytes, file_name=zip_name, mime="application/zip", use_container_width=True)

else:
    st.caption("ëŒ€ê¸° ì¤‘â€¦ ë§í¬ ì…ë ¥ í›„ [í†µí•© ë¶„ì„ ì‹œì‘]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
